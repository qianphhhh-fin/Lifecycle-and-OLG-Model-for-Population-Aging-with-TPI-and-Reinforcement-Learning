"""
OLG Model V8 SAC Training Script - SBX (Stable Baselines Jax) Implementation
åŸºäºSBX (SB3 + JAX) çš„SACç®—æ³•è®­ç»ƒOLGæ¨¡å‹æ™ºèƒ½ä½“

SBXçš„ä¼˜åŠ¿:
- åŸºäºJAXï¼Œæ›´é«˜çš„è®¡ç®—æ€§èƒ½
- ä¸SB3å…¼å®¹çš„APIï¼Œæ˜“äºè¿ç§»
- æ”¯æŒç°ä»£RLç®—æ³•ï¼ˆSACã€TQCã€PPOç­‰ï¼‰
"""

import numpy as np
import jax
import pickle
import time
import os
from typing import Dict, Any, Tuple
from sbx import SAC  # ä½¿ç”¨SBXè€Œä¸æ˜¯SB3
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold
from stable_baselines3.common.monitor import Monitor
import matplotlib.pyplot as plt

from olg_utils import OLGUtils
from olg_env_v8_sac import OLGEnvV8SAC

def evaluate_policy_with_discount(model, env, n_eval_episodes=10, deterministic=True, 
                                 gamma=0.97, render=False, callback=None, 
                                 return_episode_rewards=False):
    """
    è‡ªå®šä¹‰è¯„ä¼°å‡½æ•°ï¼Œä½¿ç”¨æŒ‡å®šçš„æŠ˜ç°å› å­è®¡ç®—episodeæ€»å›æŠ¥
    è§£å†³SBX/SB3çš„evaluate_policyé»˜è®¤æ— æŠ˜ç°(Î³=1.0)çš„é—®é¢˜
    
    Args:
        model: è¦è¯„ä¼°çš„æ¨¡å‹
        env: ç¯å¢ƒ
        n_eval_episodes: è¯„ä¼°å›åˆæ•°
        deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
        gamma: æŠ˜ç°å› å­ï¼ˆåº”ä¸è®­ç»ƒæ—¶çš„gammaä¿æŒä¸€è‡´ï¼‰
        render: æ˜¯å¦æ¸²æŸ“
        callback: å›è°ƒå‡½æ•°
        return_episode_rewards: æ˜¯å¦è¿”å›æ¯ä¸ªepisodeçš„å¥–åŠ±
        
    Returns:
        mean_reward: å¹³å‡æŠ˜ç°å›æŠ¥
        std_reward: æ ‡å‡†å·®
        episode_rewards: (å¯é€‰) æ¯ä¸ªepisodeçš„æŠ˜ç°å›æŠ¥åˆ—è¡¨
    """
    episode_rewards = []
    episode_lengths = []
    
    for i in range(n_eval_episodes):
        # å…¼å®¹ä¸åŒçš„reset()è¿”å›å€¼æ ¼å¼
        reset_result = env.reset()
        if isinstance(reset_result, tuple):
            obs, _ = reset_result
        else:
            obs = reset_result
            
        done = False
        episode_reward = 0.0
        episode_length = 0
        discount_factor = 1.0
        
        while not done:
            action, _ = model.predict(obs, deterministic=deterministic)
            
            # å…¼å®¹ä¸åŒçš„step()è¿”å›å€¼æ ¼å¼
            step_result = env.step(action)
            if len(step_result) == 5:
                obs, reward, terminated, truncated, info = step_result
                done = terminated or truncated
            elif len(step_result) == 4:
                obs, reward, done, info = step_result
            else:
                raise ValueError(f"Unexpected step() return format: {len(step_result)} values")
            
            # ä½¿ç”¨æŠ˜ç°å› å­ç´¯ç§¯å¥–åŠ±
            episode_reward += discount_factor * reward
            discount_factor *= gamma
            
            episode_length += 1
            
            if render:
                env.render()
            
            if callback is not None:
                callback(locals(), globals())
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
    
    mean_reward = np.mean(episode_rewards)
    std_reward = np.std(episode_rewards)
    
    if return_episode_rewards:
        return mean_reward, std_reward, episode_rewards
    else:
        return mean_reward, std_reward

def evaluate_policy_lifecycle_simulation(model, cS, paramS_for_rl, rng_M, 
                                        n_sim=50, deterministic=True, 
                                        gamma=0.97, random_seed=42, verbose=True):
    """
    ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿè¯„ä¼°RLæ¨¡å‹ï¼ˆä¸compare_rl_and_vfi.pyå’ŒMATLABç‰ˆæœ¬å®Œå…¨ä¸€è‡´çš„è¯„ä¼°è¿‡ç¨‹ï¼‰
    
    ğŸš¨ é‡è¦æ›´æ–°ï¼šæœ€åä¸€æœŸå¤„ç†ä¸MATLABç‰ˆæœ¬å®Œå…¨ä¸€è‡´
    - éæœ€åä¸€æœŸï¼šä½¿ç”¨RLç­–ç•¥è¿›è¡Œå†³ç­–
    - æœ€åä¸€æœŸï¼šå¼ºåˆ¶æ¶ˆè´¹æ‰€æœ‰èµ„äº§ï¼Œä¸å‚¨è“„ï¼Œä¸ç¼´è´¹PPSï¼ˆä¸main_olg_v8_utils.mä¸€è‡´ï¼‰
    
    Args:
        model: è¦è¯„ä¼°çš„æ¨¡å‹
        cS: æ¨¡å‹å‚æ•°
        paramS_for_rl: RLä¸“ç”¨å‚æ•°
        rng_M: å®è§‚å‚æ•°èŒƒå›´ï¼ˆç”¨äºç¯å¢ƒåˆå§‹åŒ–ï¼‰
        n_sim: æ¨¡æ‹Ÿä¸ªä½“æ•°é‡
        deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
        gamma: æŠ˜ç°å› å­
        random_seed: éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡ç°
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        
    Returns:
        mean_reward: å¹³å‡ç”Ÿå‘½å‘¨æœŸæŠ˜ç°æ•ˆç”¨
        std_reward: æ ‡å‡†å·®
        lifecycle_results: è¯¦ç»†çš„ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿç»“æœ
    
    å¤„ç†é€»è¾‘:
        1. å¹´é¾„0åˆ°aD_new-2ï¼šä½¿ç”¨RLæ¨¡å‹çš„ç­–ç•¥å†³ç­–
        2. å¹´é¾„aD_new-1ï¼ˆæœ€åä¸€æœŸï¼‰ï¼šå¼ºåˆ¶æ¶ˆè´¹æ‰€æœ‰èµ„äº§ï¼Œä¸å‚¨è“„ï¼Œä¸ç¼´è´¹PPS
           - k_next = kMin (ä¸å‚¨è“„)
           - c_pps = 0 (ä¸ç¼´è´¹PPS)
           - consumption = æ‰€æœ‰å¯ç”¨èµ„æº / (1 + tau_c)
           - utility = CRRA(consumption)
    """
    # ğŸ¯ ä½¿ç”¨ä¸compare_rl_and_vfi.pyå®Œå…¨ç›¸åŒçš„å›ºå®šæµ‹è¯•å‚æ•°
    M_fixed = {
        'R_k_net_factor': 1.03,
        'w_gross': 2.0,
        'TR_total': 0.1,
        'b_payg_avg_retiree': 0.4,
        'tau_l': 0.15,
        'theta_payg_actual': 0.12
    }
    
    if verbose:
        print(f"\nğŸ§¬ ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿè¯„ä¼° (n_sim={n_sim}, seed={random_seed})")
        print("ğŸ¯ ä½¿ç”¨ä¸compare_rl_and_vfi.pyå®Œå…¨ç›¸åŒçš„è¯„ä¼°é€»è¾‘")
        print("ğŸ“Š å›ºå®šæµ‹è¯•å‚æ•°:")
        for key, value in M_fixed.items():
            print(f"  {key} = {value:.3f}")
    
    # ğŸ² è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
    np.random.seed(random_seed)
    
    # è·å–ç»´åº¦å‚æ•°
    aD_new = int(cS['aD_new'])
    aR_new = int(cS['aR_new'])
    nk = int(cS['nk'])
    nkpps = int(cS['nkpps'])
    nw = int(cS['nw'])
    
    # ä»å‚æ•°ä¸­æå–ç½‘æ ¼å’Œè½¬ç§»æ¦‚ç‡
    kGridV = np.array(cS['kGridV']).flatten()
    kppsGridV = np.array(cS['kppsGridV']).flatten()
    leGridV = np.array(paramS_for_rl['leGridV']).flatten()
    leTrProbM = np.array(paramS_for_rl['leTrProbM'])
    leProb1V = np.array(paramS_for_rl['leProb1V']).flatten()
    ageEffV_new = np.array(cS['ageEffV_new']).flatten()
    beta = float(cS['beta'])
    sigma = float(cS['sigma'])
    kMin = float(cS['kMin'])
    kppsMin = float(cS['kppsMin'])
    kMax = float(cS['kMax'])
    kppsMax = float(cS['kppsMax'])
    pps_active = bool(cS['pps_active'])
    
    if verbose:
        print(f"ğŸ“Š æ¨¡å‹ç»´åº¦: aD_new={aD_new}, nk={nk}, nkpps={nkpps}, nw={nw}")
        print(f"ğŸ“Š æŠ˜ç°å› å­: Î²={beta:.3f} (åº”ä¸Î³={gamma:.3f}ä¸€è‡´)")
    
    # åˆ›å»ºä¸“ç”¨è¯„ä¼°ç¯å¢ƒï¼ˆä¸compare_rl_and_vfi.pyä¸­çš„è®¾ç½®å®Œå…¨ç›¸åŒï¼‰
    env = OLGEnvV8SAC(cS, paramS_for_rl, rng_M)
    env.set_macro_parameters(M_fixed)
    
    # åˆå§‹åŒ–ç»“æœå­˜å‚¨
    lifetime_utility_rl = np.zeros(n_sim)
    
    # ç”Ÿå‘½å‘¨æœŸè½¨è¿¹å­˜å‚¨
    k_path_rl = np.zeros((n_sim, aD_new))
    c_path_rl = np.zeros((n_sim, aD_new))
    cpps_path_rl = np.zeros((n_sim, aD_new))
    
    if verbose:
        print("ğŸ”„ å¼€å§‹ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿ...")
    
    sim_start_time = time.time()
    
    for i_sim in range(n_sim):
        if verbose and (i_sim + 1) % 10 == 0:
            print(f"  è¿›åº¦: {i_sim + 1}/{n_sim}")
        
        # åˆå§‹çŠ¶æ€
        k_current_rl = kMin
        kpps_current_rl = kppsMin
        
        # åˆå§‹æ•ˆç‡å†²å‡»ï¼ˆä¸compare_rl_and_vfi.pyå®Œå…¨ç›¸åŒçš„é€»è¾‘ï¼‰
        eps_idx_current = np.where(np.random.rand() <= np.cumsum(leProb1V))[0]
        if len(eps_idx_current) > 0:
            eps_idx_current = eps_idx_current[0]
        else:
            eps_idx_current = 0
        
        utility_sum_rl = 0
        
        # RLç¯å¢ƒé‡ç½®
        obs, _ = env.reset()
        env.set_macro_parameters(M_fixed)
        
        for age_idx in range(aD_new):
            # ğŸš¨ æœ€åä¸€æœŸç‰¹æ®Šå¤„ç†ï¼šä¸MATLABç‰ˆæœ¬å®Œå…¨ä¸€è‡´
            if age_idx == aD_new - 1:
                # æœ€åä¸€æœŸï¼šå¼ºåˆ¶æ¶ˆè´¹æ‰€æœ‰èµ„äº§ï¼Œä¸å‚¨è“„ï¼Œä¸ç¼´è´¹PPS
                if verbose and i_sim == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªæ¨¡æ‹Ÿæ—¶æ‰“å°
                    print(f"    ğŸ æœ€åä¸€æœŸ (age_idx={age_idx}): å¼ºåˆ¶æ¶ˆè´¹æ‰€æœ‰èµ„äº§")
                
                # è®¡ç®—å¯ç”¨èµ„æºï¼ˆä¸MATLABçš„HHIncome_Huggetté€»è¾‘ä¸€è‡´ï¼‰
                # åŸºç¡€æ”¶å…¥ï¼šèµ„æœ¬æ”¶å…¥ + åŠ³åŠ¨æ”¶å…¥ + è½¬ç§»æ”¯ä»˜ + PAYGæ”¶ç›Š
                r_k_net = M_fixed['R_k_net_factor'] - 1  # å‡€åˆ©ç‡
                capital_income = k_current_rl * r_k_net
                
                # åŠ³åŠ¨æ”¶å…¥ï¼ˆæœ€åä¸€æœŸé€šå¸¸æ˜¯é€€ä¼‘æœŸï¼ŒåŠ³åŠ¨æ•ˆç‡ä¸º0ï¼‰
                age_efficiency = ageEffV_new[min(age_idx, len(ageEffV_new)-1)]
                epsilon_val = leGridV[eps_idx_current]
                labor_income = M_fixed['w_gross'] * age_efficiency * epsilon_val
                
                # è½¬ç§»æ”¯ä»˜å’ŒPAYGæ”¶ç›Š
                transfer_income = M_fixed['TR_total']
                payg_income = M_fixed['b_payg_avg_retiree'] if age_idx >= aR_new else 0
                
                # æ€»çš„éPPSèµ„æº
                total_non_pps_resources = capital_income + labor_income + transfer_income + payg_income
                
                # PPSèµ„æºï¼ˆå¦‚æœæ¿€æ´»ä¸”åœ¨æå–æœŸï¼‰
                pps_withdrawal = 0
                if pps_active and age_idx >= aR_new:  # é€€ä¼‘æœŸå¯ä»¥æå–PPS
                    pps_tax_rate_withdrawal = float(cS.get('pps_tax_rate_withdrawal', 0.03))
                    pps_withdrawal = kpps_current_rl * (1 - pps_tax_rate_withdrawal)
                
                # æ€»å¯ç”¨èµ„æº
                total_available_resources = total_non_pps_resources + pps_withdrawal
                
                # å¼ºåˆ¶è®¾ç½®å†³ç­–å˜é‡ï¼ˆä¸MATLABä¸€è‡´ï¼‰
                k_next_rl = kMin  # ä¸å‚¨è“„
                cpps_rl = 0       # ä¸ç¼´è´¹PPS
                
                # è®¡ç®—æ¶ˆè´¹ï¼ˆæ‰£é™¤æ¶ˆè´¹ç¨ï¼‰
                tau_c = float(cS.get('tau_c', 0.10))
                c_rl = max(float(cS.get('cFloor', 0.05)), total_available_resources / (1 + tau_c))
                
                # è®¡ç®—æ•ˆç”¨ï¼ˆç›´æ¥åŸºäºæ¶ˆè´¹è®¡ç®—ï¼Œä¸MATLABçš„CES_utilityä¸€è‡´ï¼‰
                sigma_val = float(cS['sigma'])
                if abs(sigma_val - 1.0) < 1e-6:
                    u_rl = np.log(max(c_rl, 1e-10))  # logæ•ˆç”¨
                else:
                    u_rl = (max(c_rl, 1e-10)**(1 - sigma_val)) / (1 - sigma_val)  # CRRAæ•ˆç”¨
                
                if verbose and i_sim == 0:
                    print(f"      ğŸ’° æ€»å¯ç”¨èµ„æº: {total_available_resources:.4f}")
                    print(f"      ğŸ½ï¸  æœ€åä¸€æœŸæ¶ˆè´¹: {c_rl:.4f}")
                    print(f"      ğŸ“Š æœ€åä¸€æœŸæ•ˆç”¨: {u_rl:.4f}")
                    print(f"      ğŸ’¾ å‚¨è“„: {k_next_rl:.4f} (å¼ºåˆ¶ä¸ºkMin)")
                    print(f"      ğŸ¦ PPSç¼´è´¹: {cpps_rl:.4f} (å¼ºåˆ¶ä¸º0)")
                
            else:
                # éæœ€åä¸€æœŸï¼šä½¿ç”¨RLç­–ç•¥
                action, _ = model.predict(obs, deterministic=deterministic)
                next_obs, reward, terminated, truncated, info = env.step(action)
                
                c_rl = info.get('consumption', 0)
                k_next_rl = info.get('k_prime', 0)
                cpps_rl = info.get('c_pps', 0)
                
                # RLçš„æ•ˆç”¨ç›´æ¥æ¥è‡ªrewardï¼ˆä¸compare_rl_and_vfi.pyä¸€è‡´ï¼‰
                u_rl = reward
                
                obs = next_obs
            
            # å­˜å‚¨è½¨è¿¹ï¼ˆé€‚ç”¨äºæ‰€æœ‰å¹´é¾„ï¼‰
            k_path_rl[i_sim, age_idx] = k_current_rl
            c_path_rl[i_sim, age_idx] = c_rl
            cpps_path_rl[i_sim, age_idx] = cpps_rl
            
            # ç´¯ç§¯æŠ˜ç°æ•ˆç”¨ï¼ˆä¸compare_rl_and_vfi.pyå®Œå…¨ç›¸åŒçš„é€»è¾‘ï¼‰
            discount_factor = beta ** age_idx
            utility_sum_rl += discount_factor * u_rl
            
            # çŠ¶æ€æ›´æ–°
            k_current_rl = k_next_rl
            
            # PPSèµ„äº§æ¼”åŒ–ï¼ˆç®€åŒ–å¤„ç†ï¼Œä¸compare_rl_and_vfi.pyä¸€è‡´ï¼‰
            if pps_active:
                R_k_net_factor = M_fixed['R_k_net_factor']
                pps_return_premium = float(cS.get('pps_return_rate_premium', 0))
                pps_return_factor = 1 + ((R_k_net_factor - 1) + pps_return_premium)
                
                kpps_current_rl = (kpps_current_rl + cpps_rl) * pps_return_factor
                kpps_current_rl = max(kppsMin, min(kppsMax, kpps_current_rl))
            
            # æ•ˆç‡å†²å‡»æ¼”åŒ–ï¼ˆä¸compare_rl_and_vfi.pyå®Œå…¨ç›¸åŒçš„é€»è¾‘ï¼‰
            if age_idx < aD_new - 1:
                trans_probs = leTrProbM[eps_idx_current, :]
                eps_idx_current = np.where(np.random.rand() <= np.cumsum(trans_probs))[0]
                if len(eps_idx_current) > 0:
                    eps_idx_current = eps_idx_current[0]
                else:
                    eps_idx_current = min(eps_idx_current, nw-1)
            
            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶ï¼ˆä»…å¯¹éæœ€åä¸€æœŸï¼‰
            if age_idx < aD_new - 1 and (terminated or truncated):
                break
        
        lifetime_utility_rl[i_sim] = utility_sum_rl
    
    sim_time = time.time() - sim_start_time
    
    # è®¡ç®—ç»Ÿè®¡ç»“æœ
    mean_utility_rl = np.mean(lifetime_utility_rl)
    std_utility_rl = np.std(lifetime_utility_rl)
    
    if verbose:
        print(f"âœ… ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿå®Œæˆï¼Œè€—æ—¶: {sim_time:.2f} ç§’")
        print(f"ğŸ“Š RLç”Ÿå‘½å‘¨æœŸæ•ˆç”¨: {mean_utility_rl:.4f} Â± {std_utility_rl:.4f}")
    
    # æ„é€ è¯¦ç»†ç»“æœ
    lifecycle_results = {
        'success': True,
        'n_sim': n_sim,
        'random_seed': random_seed,
        'M_fixed': M_fixed,
        'simulation_time': sim_time,
        'mean_utility_rl': mean_utility_rl,
        'std_utility_rl': std_utility_rl,
        'lifetime_utility_rl': lifetime_utility_rl,
        'k_path_rl': k_path_rl,
        'c_path_rl': c_path_rl,
        'cpps_path_rl': cpps_path_rl,
        'aD_new': aD_new,
        'beta': beta,
        'gamma': gamma
    }
    
    return mean_utility_rl, std_utility_rl, lifecycle_results

class EvalCallbackWithDiscount(EvalCallback):
    """
    è‡ªå®šä¹‰EvalCallbackï¼Œä½¿ç”¨ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿè¿›è¡Œè¯„ä¼°
    ğŸ†• ä½¿ç”¨evaluate_policy_lifecycle_simulationç¡®ä¿ä¸æœ€ç»ˆè¯„ä¼°å®Œå…¨ä¸€è‡´
    """
    
    def __init__(self, eval_env, cS, paramS_for_rl, rng_M, gamma=0.97, 
                 use_lifecycle_simulation=True, **kwargs):
        """
        Args:
            eval_env: è¯„ä¼°ç¯å¢ƒ
            cS: æ¨¡å‹å‚æ•°
            paramS_for_rl: RLä¸“ç”¨å‚æ•°
            rng_M: å®è§‚å‚æ•°èŒƒå›´
            gamma: æŠ˜ç°å› å­ï¼Œåº”ä¸è®­ç»ƒæ—¶çš„gammaä¿æŒä¸€è‡´
            use_lifecycle_simulation: æ˜¯å¦ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿï¼ˆé»˜è®¤Trueï¼‰
            **kwargs: å…¶ä»–EvalCallbackå‚æ•°
        """
        super().__init__(eval_env, **kwargs)
        self.gamma = gamma
        self.cS = cS
        self.paramS_for_rl = paramS_for_rl
        self.rng_M = rng_M
        self.use_lifecycle_simulation = use_lifecycle_simulation
        
        if use_lifecycle_simulation:
            print(f"ğŸ”§ ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿè¯„ä¼°å›è°ƒ (Î³={gamma})")
            print("ğŸ¯ ç¡®ä¿ä¸evaluate_policy_lifecycle_simulationå®Œå…¨ä¸€è‡´")
        else:
            print(f"ğŸ”§ ä½¿ç”¨ä¼ ç»ŸæŠ˜ç°è¯„ä¼°å›è°ƒ (Î³={gamma})")
        
        # æµ‹è¯•ç¯å¢ƒçš„è¿”å›æ ¼å¼
        try:
            reset_result = self.eval_env.reset()
            print(f"ğŸ“‹ ç¯å¢ƒreset()è¿”å›æ ¼å¼: {type(reset_result)} - {len(reset_result) if isinstance(reset_result, tuple) else 'å•å€¼'}")
        except Exception as e:
            print(f"âš ï¸ ç¯å¢ƒæµ‹è¯•å¤±è´¥: {e}")
    
    def _on_step(self) -> bool:
        """
        é‡å†™_on_stepæ–¹æ³•ï¼Œä½¿ç”¨å¸¦æŠ˜ç°çš„è¯„ä¼°
        """
        continue_training = True

        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:
            # ä½¿ç”¨å¸¦æŠ˜ç°çš„è¯„ä¼°
            episode_rewards, episode_lengths = self._evaluate_with_discount()
            
            if self.log_path is not None:
                self.evaluations_timesteps.append(self.num_timesteps)
                self.evaluations_results.append(episode_rewards)
                self.evaluations_length.append(episode_lengths)

                kwargs = {}
                # Save success log if present
                if len(self._is_success_buffer) > 0:
                    self.evaluations_successes.append(self._is_success_buffer)
                    kwargs = dict(successes=self.evaluations_successes)

                np.savez(
                    self.log_path,
                    timesteps=self.evaluations_timesteps,
                    results=self.evaluations_results,
                    ep_lengths=self.evaluations_length,
                    **kwargs,
                )

            mean_reward, std_reward = np.mean(episode_rewards), np.std(episode_rewards)
            mean_ep_length, std_ep_length = np.mean(episode_lengths), np.std(episode_lengths)
            self.last_mean_reward = mean_reward

            if self.verbose >= 1:
                print(f"ğŸ“Š Eval num_timesteps={self.num_timesteps}")
                print(f"   Current reward: {mean_reward:.2f} +/- {std_reward:.2f} (æŠ˜ç°)")
                print(f"   Episode length: {mean_ep_length:.2f} +/- {std_ep_length:.2f}")
                print(f"ğŸ† Best reward: {self.best_mean_reward:.2f} (at timestep {getattr(self, 'best_timestep', 'N/A')})")
                
                # æ˜¾ç¤ºæ”¹è¿›æƒ…å†µ
                if hasattr(self, 'best_timestep'):
                    improvement = mean_reward - self.best_mean_reward
                    if improvement > 0:
                        print(f"   ğŸ“ˆ Improvement: +{improvement:.2f}")
                    elif improvement < 0:
                        print(f"   ğŸ“‰ Below best: {improvement:.2f}")
                    else:
                        print(f"   â– Same as best")

            # Add to current Logger
            self.logger.record("eval/mean_reward", float(mean_reward))
            self.logger.record("eval/mean_ep_length", mean_ep_length)

            if len(self._is_success_buffer) > 0:
                success_rate = np.mean(self._is_success_buffer)
                if self.verbose >= 1:
                    print(f"Success rate: {100 * success_rate:.2f}%")
                self.logger.record("eval/success_rate", success_rate)

            # Dump log so the evaluation results are printed with the correct timestep
            self.logger.record("time/total_timesteps", self.num_timesteps, exclude="tensorboard")
            self.logger.dump(self.num_timesteps)

            if mean_reward > self.best_mean_reward:
                if self.verbose >= 1:
                    print("ğŸ† New best mean reward!")
                if self.best_model_save_path is not None:
                    self.model.save(os.path.join(self.best_model_save_path, "best_model"))
                self.best_mean_reward = mean_reward
                self.best_timestep = self.num_timesteps  # è®°å½•æœ€ä½³ç»“æœçš„timestep

                # Trigger callback on new best model, if needed
                if self.callback_on_new_best is not None:
                    continue_training = self.callback_on_new_best.on_step()

        return continue_training
    
    def _evaluate_with_discount(self):
        """ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿæˆ–ä¼ ç»ŸæŠ˜ç°è¿›è¡Œè¯„ä¼°"""
        if self.use_lifecycle_simulation:
            return self._evaluate_with_lifecycle_simulation()
        else:
            return self._evaluate_with_traditional_discount()
    
    def _evaluate_with_lifecycle_simulation(self):
        """ğŸ†• ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿè¿›è¡Œè¯„ä¼°"""
        # ç”ŸæˆåŸºäºtimestepçš„éšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡è¯„ä¼°çš„å¯é‡ç°æ€§
        eval_random_seed = 42 + (self.num_timesteps // 1000)  # åŸºäºè®­ç»ƒæ­¥æ•°çš„ç§å­
        
        # è°ƒç”¨ç»Ÿä¸€çš„ç”Ÿå‘½å‘¨æœŸè¯„ä¼°å‡½æ•°
        mean_reward, std_reward, lifecycle_results = evaluate_policy_lifecycle_simulation(
            self.model, self.cS, self.paramS_for_rl, self.rng_M,
            n_sim=max(self.n_eval_episodes,10), 
            deterministic=self.deterministic,
            gamma=self.gamma,
            random_seed=eval_random_seed,
            verbose=False  # è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        )
        
        # ç”Ÿæˆä¸ä¼ ç»Ÿæ–¹æ³•å…¼å®¹çš„è¿”å›æ ¼å¼
        # ä¸ºäº†å…¼å®¹åŸæœ‰çš„episodeæ ¼å¼ï¼Œæˆ‘ä»¬åˆ›å»ºn_eval_episodesä¸ªç›¸åŒçš„ç»“æœ
        episode_rewards = [mean_reward] * self.n_eval_episodes
        episode_lengths = [lifecycle_results['aD_new']] * self.n_eval_episodes  # ç”Ÿå‘½å‘¨æœŸé•¿åº¦
        
        if self.verbose >= 1:
            print(f"   ğŸ§¬ ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿ (n_sim={lifecycle_results['n_sim']}, seed={eval_random_seed})")
            print(f"   ğŸ“Š æ¨¡æ‹Ÿç»“æœ: {mean_reward:.4f} Â± {std_reward:.4f}")
        
        return episode_rewards, episode_lengths
    
    def _evaluate_with_traditional_discount(self):
        """ä¼ ç»Ÿçš„æŠ˜ç°è¯„ä¼°æ–¹æ³•ï¼ˆå¤‡ç”¨ï¼‰"""
        episode_rewards = []
        episode_lengths = []
        
        for i in range(self.n_eval_episodes):
            # å…¼å®¹ä¸åŒçš„reset()è¿”å›å€¼æ ¼å¼
            reset_result = self.eval_env.reset()
            if isinstance(reset_result, tuple):
                obs, _ = reset_result
            else:
                obs = reset_result
                
            done = False
            episode_reward = 0.0
            episode_length = 0
            discount_factor = 1.0
            
            while not done:
                action, _ = self.model.predict(obs, deterministic=self.deterministic)
                
                # å…¼å®¹ä¸åŒçš„step()è¿”å›å€¼æ ¼å¼
                step_result = self.eval_env.step(action)
                if len(step_result) == 5:
                    obs, reward, terminated, truncated, info = step_result
                    done = terminated or truncated
                elif len(step_result) == 4:
                    obs, reward, done, info = step_result
                else:
                    raise ValueError(f"Unexpected step() return format: {len(step_result)} values")
                
                # ä½¿ç”¨æŠ˜ç°å› å­ç´¯ç§¯å¥–åŠ±
                episode_reward += discount_factor * reward
                discount_factor *= self.gamma
                
                episode_length += 1
                
                if self.render:
                    self.eval_env.render()
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
        
        return episode_rewards, episode_lengths

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("=== OLG æ¨¡å‹ V8 - SAC æ™ºèƒ½ä½“è®­ç»ƒ (SBX - Stable Baselines Jax ç‰ˆæœ¬) ===")
    print("    (åœ¨çº¿RLï¼Œå®è§‚å˜é‡Mä½œä¸ºç¯å¢ƒå‚æ•°)")
    print("    (å†³ç­–å˜é‡ï¼šPPSç¼´è´¹æ¯”ä¾‹, éPPSå‚¨è“„æ¯”ä¾‹)")
    print("    (ç½‘ç»œæ¶æ„ï¼šSBX JAX SACå®ç°)")
    print(f"    (JAXè®¾å¤‡: {jax.devices()})")
    
    # 1. åˆå§‹åŒ–å‚æ•°
    print("\n--- 1. åˆå§‹åŒ–å‚æ•° ---")
    cS = OLGUtils.parameter_values_huggett_style()
    
    # è®¡ç®—RLç›¸å…³å‚æ•°
    paramS_for_rl = {}
    if ('leGridV' not in cS or 'leTrProbM' not in cS or 'leProb1V' not in cS):
        (paramS_for_rl['leLogGridV'], 
         paramS_for_rl['leTrProbM'], 
         paramS_for_rl['leProb1V']) = OLGUtils.earning_process_olgm(cS)
        paramS_for_rl['leGridV'] = np.exp(paramS_for_rl['leLogGridV'])
        cS['leGridV'] = paramS_for_rl['leGridV']
        cS['leTrProbM'] = paramS_for_rl['leTrProbM']
        cS['leProb1V'] = paramS_for_rl['leProb1V']
    else:
        paramS_for_rl['leGridV'] = cS['leGridV']
        paramS_for_rl['leTrProbM'] = cS['leTrProbM']
        paramS_for_rl['leProb1V'] = cS['leProb1V']
    
    paramS_for_rl['ageEffV_new'] = cS['ageEffV_new']
    
    # 2. å®šä¹‰å®è§‚çŠ¶æ€Mçš„é‡‡æ ·èŒƒå›´
    print("\n--- 2. å®šä¹‰å®è§‚çŠ¶æ€ M çš„é‡‡æ ·èŒƒå›´ ---")
    rng_M = {
        'R_k_net_factor': [1.01, 1.05],
        'w_gross': [1.5, 2.5],
        'TR_total': [0.0, 0.2],
        'b_payg_avg_retiree': [0.1, 0.8],
        'tau_l': [0.05, 0.25],
        'theta_payg_actual': [0.05, 0.20]
    }
    print("å®è§‚å‚æ•°é‡‡æ ·èŒƒå›´å·²å®šä¹‰ã€‚")
    
    # 3. åˆ›å»ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
    print("\n--- 3. åˆ›å»ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒ ---")
    env = OLGEnvV8SAC(cS, paramS_for_rl, rng_M)
    env = Monitor(env)  # åŒ…è£…ç¯å¢ƒä»¥è®°å½•ç»Ÿè®¡ä¿¡æ¯
    
    print(f"è§‚æµ‹ç©ºé—´: {env.observation_space}")
    print(f"åŠ¨ä½œç©ºé—´: {env.action_space}")
    print("RLç¯å¢ƒå·²åˆ›å»ºã€‚")
    
    # 4. åˆ›å»ºSBX SAC Agent
    print("\n--- 4. åˆ›å»º SBX SAC Agent ---")
    
    # SBX SACè¶…å‚æ•°ï¼ˆJAXä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    model_kwargs = {
        'policy': 'MlpPolicy',
        'env': env,
        'learning_rate': 1e-4,           # SBXæ¨èçš„å­¦ä¹ ç‡
        'buffer_size': int(1e6),         # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
        'batch_size': 512,               # SBXæ¨èçš„æ‰¹é‡å¤§å°
        'tau': 5e-3,                     # ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°ç³»æ•°
        'gamma': 0.97,                   # æŠ˜æ‰£å› å­
        'ent_coef': 'auto',              # è‡ªåŠ¨è°ƒæ•´ç†µç³»æ•°
        'gradient_steps': 1,             # æ¯æ¬¡æ›´æ–°çš„æ¢¯åº¦æ­¥æ•°
        'policy_kwargs': {
            'net_arch': [256, 256],      # ç½‘ç»œæ¶æ„
            # SBXä½¿ç”¨JAXï¼Œä¸éœ€è¦æŒ‡å®šactivation_fn
        },
        'verbose': 1,
        'learning_starts': 10000,
        'seed': 42,
        'tensorboard_log': './py/tensorboard_logs_sbx/'  # ğŸ“Š TensorBoardæ—¥å¿—è·¯å¾„
        # æ³¨æ„ï¼šSBXä¸æ”¯æŒtarget_update_intervalå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
    }
    
    print("åˆ›å»ºSBX SACæ¨¡å‹...")
    model = SAC(**model_kwargs)
    print("SBX SAC Agentå·²åˆ›å»ºã€‚")
    print(f"ä½¿ç”¨JAXè®¾å¤‡: {jax.devices()}")
    print(f"ç½‘ç»œæ¶æ„: {model_kwargs['policy_kwargs']}")
    
    # 5. è®¾ç½®è®­ç»ƒå‚æ•°
    print("\n--- 5. è®¾ç½®è®­ç»ƒå‚æ•° ---")
    max_steps_per_episode = cS['aD_new']    # æ¯å›åˆæœ€å¤§æ­¥æ•°
    total_timesteps = 150_000               # æ€»è®­ç»ƒæ­¥æ•°ï¼ˆé€‚ä¸­çš„æ•°é‡ç”¨äºæµ‹è¯•ï¼‰
    stop_training_value = -20               # åœæ­¢è®­ç»ƒé˜ˆå€¼
    eval_freq = 5_000                       # è¯„ä¼°é¢‘ç‡
    n_eval_episodes = 100                    # è¯„ä¼°å›åˆæ•°
    
    print(f"æ¯å›åˆæœ€å¤§æ­¥æ•°: {max_steps_per_episode}")
    print(f"æ€»è®­ç»ƒæ­¥æ•°: {total_timesteps}")
    print(f"åœæ­¢è®­ç»ƒå€¼: {stop_training_value}")
    
    # 6. æµ‹è¯•ç¯å¢ƒå’Œæ™ºèƒ½ä½“åˆå§‹åŒ–
    print("\n--- 6. æµ‹è¯•ç¯å¢ƒå’Œæ™ºèƒ½ä½“åˆå§‹åŒ– ---")
    obs, _ = env.reset()
    print(f"ç¯å¢ƒé‡ç½®æˆåŠŸï¼Œè§‚å¯Ÿç»´åº¦: {obs.shape}")
    
    # æµ‹è¯•æ™ºèƒ½ä½“çš„åˆå§‹åŠ¨ä½œ
    action, _ = model.predict(obs, deterministic=False)
    print(f"æ™ºèƒ½ä½“åˆå§‹åŠ¨ä½œç”ŸæˆæˆåŠŸï¼ŒåŠ¨ä½œç»´åº¦: {action.shape}ï¼ŒåŠ¨ä½œå€¼: [{action[0]:.4f}, {action[1]:.4f}]")
    
    # 7. è®¾ç½®è¯„ä¼°å›è°ƒ
    print("\n--- 7. è®¾ç½®è¯„ä¼°å’Œå›è°ƒ ---")
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒï¼ˆä½¿ç”¨å›ºå®šå‚æ•°ï¼‰
    eval_env_base = OLGEnvV8SAC(cS, paramS_for_rl, rng_M)
    M_eval = {
        'R_k_net_factor': 1.03,
        'w_gross': 2.0,
        'TR_total': 0.1,
        'b_payg_avg_retiree': 0.4,
        'tau_l': 0.15,
        'theta_payg_actual': 0.12
    }
    eval_env_base.set_macro_parameters(M_eval)
    eval_env = Monitor(eval_env_base)
    
    # è®¾ç½®åœæ­¢è®­ç»ƒå›è°ƒ
    stop_callback = StopTrainingOnRewardThreshold(
        reward_threshold=stop_training_value, 
        verbose=1
    )
    
    # è®¾ç½®è¯„ä¼°å›è°ƒ
    eval_callback = EvalCallbackWithDiscount(
        eval_env,
        cS,                              # ğŸ†• ä¼ é€’æ¨¡å‹å‚æ•°
        paramS_for_rl,                   # ğŸ†• ä¼ é€’RLä¸“ç”¨å‚æ•°
        rng_M,                           # ğŸ†• ä¼ é€’å®è§‚å‚æ•°èŒƒå›´
        gamma=0.97,                      # ğŸ”§ é‡è¦ï¼šä½¿ç”¨ä¸è®­ç»ƒä¸€è‡´çš„æŠ˜ç°å› å­
        use_lifecycle_simulation=True,   # ğŸ†• ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿ
        best_model_save_path='./py/best_model_sbx/',
        log_path='./py/logs_sbx/',
        eval_freq=eval_freq,
        n_eval_episodes=n_eval_episodes,
        deterministic=True,
        render=False,
        callback_on_new_best=stop_callback,
        verbose=1
    )
    
    print("ğŸ§¬ ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿè¯„ä¼°å›è°ƒå·²è®¾ç½® (Î³=0.97)ã€‚")
    print("ğŸ¯ è®­ç»ƒè¿‡ç¨‹è¯„ä¼°å°†ä¸æœ€ç»ˆè¯„ä¼°ä½¿ç”¨å®Œå…¨ç›¸åŒçš„é€»è¾‘ã€‚")
    
    # 8. å¼€å§‹è®­ç»ƒ
    print("\n--- 8. å¼€å§‹è®­ç»ƒ ---")
    print("ä½¿ç”¨SBX (Stable Baselines Jax) SACç®—æ³•è®­ç»ƒ...")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs('./py/best_model_sbx/', exist_ok=True)
    os.makedirs('./py/logs_sbx/', exist_ok=True)
    os.makedirs('./py/tensorboard_logs_sbx/', exist_ok=True)  # ğŸ“Š TensorBoardæ—¥å¿—ç›®å½•
    
    start_time = time.time()
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=eval_callback,
            log_interval=100,
            progress_bar=True
        )
        print("è®­ç»ƒå®Œæˆã€‚")
    except KeyboardInterrupt:
        print("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ã€‚")
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    training_time = time.time() - start_time
    print(f"è®­ç»ƒç”¨æ—¶: {training_time:.2f} ç§’")
    
    # 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\n--- 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---")
    model.save('./py/final_sac_agent_olg_sbx')

    # ä¿å­˜å‚æ•°å’Œç¯å¢ƒé…ç½®
    config = {
        'cS': cS,
        'paramS_for_rl': paramS_for_rl,
        'rng_M': rng_M,
        'M_eval': M_eval,
        'model_kwargs': model_kwargs,
        'training_time': training_time,
        'algorithm': 'SBX_SAC',
        'jax_devices': str(jax.devices())
    }

    # å¯¼å‡ºActorç½‘ç»œä¸ºONNXæ ¼å¼
    print("\n--- 9.1 å¯¼å‡ºActorç½‘ç»œä¸ºONNXæ ¼å¼ ---")
    try:
        import torch as th
        import onnx
        import json
        
        class OnnxableSACPolicy(th.nn.Module):
            """ç”¨äºONNXå¯¼å‡ºçš„åŒ…è£…ç±»"""
            def __init__(self, model):
                super().__init__()
                self.model = model
                
            def forward(self, observation: th.Tensor) -> th.Tensor:
                """åªå¯¼å‡ºactorç½‘ç»œçš„å‰å‘ä¼ æ’­"""
                # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
                with th.no_grad():
                    # è¿™é‡Œæˆ‘ä»¬åªéœ€è¦åŠ¨ä½œï¼Œä¸éœ€è¦å…¶ä»–è¿”å›å€¼
                    action = self.model.predict(observation.numpy(), deterministic=True)[0]
                    return th.as_tensor(action)
        
        # æå–æ¨¡å‹å‚æ•°
        model_params = {}
        
        # è·å–è§‚å¯Ÿç©ºé—´å’ŒåŠ¨ä½œç©ºé—´ä¿¡æ¯
        observation_space = model.observation_space
        action_space = model.action_space
        
        # æå–ç»´åº¦ä¿¡æ¯
        if hasattr(observation_space, 'shape'):
            model_params['input_dim'] = observation_space.shape[0]
        else:
            model_params['input_dim'] = 1
        
        if hasattr(action_space, 'shape'):
            model_params['output_dim'] = action_space.shape[0]
        else:
            model_params['output_dim'] = 1
        
        # æå–åŠ¨ä½œç©ºé—´è¾¹ç•Œ
        if hasattr(model.action_space, 'low') and hasattr(model.action_space, 'high'):
            model_params['action_space_low'] = model.action_space.low.tolist()
            model_params['action_space_high'] = model.action_space.high.tolist()
            print(f"æå–åŠ¨ä½œç©ºé—´è¾¹ç•Œ: low={model_params['action_space_low']}, high={model_params['action_space_high']}")
        else:
            print("è­¦å‘Š: æœªèƒ½æå–åŠ¨ä½œç©ºé—´è¾¹ç•Œï¼ŒMATLABä¸­å¯èƒ½éœ€è¦æ‰‹åŠ¨è®¾ç½®ã€‚")
        
        # å°è¯•æå–ç½‘ç»œæ¶æ„ä¿¡æ¯
        try:
            if hasattr(model, 'actor') and hasattr(model.actor, 'latent_pi_net'):
                # æå–actorç½‘ç»œæ¶æ„
                actor_net = model.actor.latent_pi_net
                net_arch = []
                for module in actor_net.modules():
                    if isinstance(module, th.nn.Linear):
                        net_arch.append(module.out_features)
                model_params['net_arch'] = net_arch[:-1]  # æ’é™¤è¾“å‡ºå±‚
            else:
                # é»˜è®¤ç½‘ç»œæ¶æ„
                model_params['net_arch'] = [256, 256]
        except Exception as e:
            print(f"æå–ç½‘ç»œæ¶æ„å¤±è´¥: {e}")
            model_params['net_arch'] = [256, 256]  # é»˜è®¤å€¼
        
        # åˆ›å»ºONNXå¯¼å‡ºåŒ…è£…å™¨
        onnxable_model = OnnxableSACPolicy(model)
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        observation_size = model.observation_space.shape
        dummy_input = th.zeros(1, *observation_size)
        
        # ONNXè¾“å‡ºè·¯å¾„
        onnx_path = './py/final_sac_agent_olg_sbx.onnx'
        
        # å¯¼å‡ºåˆ°ONNX
        print(f"å¯¼å‡ºæ¨¡å‹åˆ°ONNX: {onnx_path}")
        th.onnx.export(
            onnxable_model,
            dummy_input,
            onnx_path,
            opset_version=14,
            input_names=["observation"],
            output_names=["action"],
            dynamic_axes={
                "observation": {0: "batch_size"},
                "action": {0: "batch_size"}
            }
        )
        
        # éªŒè¯ONNXæ¨¡å‹
        onnx_model = onnx.load(onnx_path)
        onnx.checker.check_model(onnx_model)
        print("ONNXæ¨¡å‹éªŒè¯é€šè¿‡")
        
        # ä»ONNXæ¨¡å‹ä¸­æå–é¢å¤–ä¿¡æ¯
        for input_info in onnx_model.graph.input:
            if input_info.name == "observation":
                # è·å–è¾“å…¥ç»´åº¦
                input_shape = []
                for dim in input_info.type.tensor_type.shape.dim:
                    if dim.dim_param:  # åŠ¨æ€ç»´åº¦
                        input_shape.append(-1)
                    else:
                        input_shape.append(dim.dim_value)
                input_shape = input_shape[1:]  # å»æ‰æ‰¹æ¬¡ç»´åº¦
                model_params['input_shape'] = input_shape
        
        for output_info in onnx_model.graph.output:
            if output_info.name == "action":
                # è·å–è¾“å‡ºç»´åº¦
                output_shape = []
                for dim in output_info.type.tensor_type.shape.dim:
                    if dim.dim_param:  # åŠ¨æ€ç»´åº¦
                        output_shape.append(-1)
                    else:
                        output_shape.append(dim.dim_value)
                output_shape = output_shape[1:]  # å»æ‰æ‰¹æ¬¡ç»´åº¦
                model_params['output_shape'] = output_shape
        
        # ä¿å­˜æ¨¡å‹å‚æ•°åˆ°JSONæ–‡ä»¶
        params_path = './py/final_sac_agent_olg_sbx_params.json'
        with open(params_path, 'w') as f:
            json.dump(model_params, f, indent=4)
        print(f"æ¨¡å‹å‚æ•°å·²ä¿å­˜åˆ°: {params_path}")
        
        # åŒæ ·ä¸ºæœ€ä½³æ¨¡å‹å¯¼å‡ºONNXå’Œå‚æ•°
        best_model_path = './py/best_model_sbx/best_model.onnx'
        best_model = SAC.load('./py/best_model_sbx/best_model')
        
        # æå–æœ€ä½³æ¨¡å‹å‚æ•°ï¼ˆä¸ä¸Šé¢ç›¸åŒçš„é€»è¾‘ï¼‰
        best_model_params = {}
        best_model_params['input_dim'] = model_params['input_dim']  # åº”è¯¥ç›¸åŒ
        best_model_params['output_dim'] = model_params['output_dim']  # åº”è¯¥ç›¸åŒ
        best_model_params['net_arch'] = model_params['net_arch']  # åº”è¯¥ç›¸åŒ
        
        # æå–æœ€ä½³æ¨¡å‹çš„åŠ¨ä½œç©ºé—´è¾¹ç•Œ
        if hasattr(best_model.action_space, 'low') and hasattr(best_model.action_space, 'high'):
            best_model_params['action_space_low'] = best_model.action_space.low.tolist()
            best_model_params['action_space_high'] = best_model.action_space.high.tolist()
        
        onnxable_best_model = OnnxableSACPolicy(best_model)
        
        print(f"å¯¼å‡ºæœ€ä½³æ¨¡å‹åˆ°ONNX: {best_model_path}")
        th.onnx.export(
            onnxable_best_model,
            dummy_input,
            best_model_path,
            opset_version=14,
            input_names=["observation"],
            output_names=["action"],
            dynamic_axes={
                "observation": {0: "batch_size"},
                "action": {0: "batch_size"}
            }
        )
        
        # éªŒè¯æœ€ä½³æ¨¡å‹çš„ONNX
        best_onnx_model = onnx.load(best_model_path)
        onnx.checker.check_model(best_onnx_model)
        print("æœ€ä½³æ¨¡å‹ONNXéªŒè¯é€šè¿‡")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹å‚æ•°
        best_params_path = './py/best_model_sbx/best_model_params.json'
        with open(best_params_path, 'w') as f:
            json.dump(best_model_params, f, indent=4)
        print(f"æœ€ä½³æ¨¡å‹å‚æ•°å·²ä¿å­˜åˆ°: {best_params_path}")
        
        # æ›´æ–°é…ç½®ä¿¡æ¯
        config['onnx_export'] = {
            'final_model': onnx_path,
            'final_model_params': params_path,
            'best_model': best_model_path,
            'best_model_params': best_params_path
        }
        
        # æ‰“å°æ¨¡å‹å‚æ•°æ‘˜è¦
        print("\næ¨¡å‹å‚æ•°æ‘˜è¦:")
        print(f"è¾“å…¥ç»´åº¦: {model_params['input_dim']}")
        print(f"è¾“å‡ºç»´åº¦: {model_params['output_dim']}")
        print(f"ç½‘ç»œæ¶æ„: {model_params['net_arch']}")
        
    except ImportError as e:
        print(f"ONNXå¯¼å‡ºå¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…PyTorchå’ŒONNXåŒ…")
    except Exception as e:
        print(f"ONNXå¯¼å‡ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    # ä¿å­˜å‚æ•°å’Œç¯å¢ƒé…ç½®
    with open('./py/training_config_sbx.pkl', 'wb') as f:
        pickle.dump(config, f)
    
    print("æœ€ç»ˆæ¨¡å‹å’Œé…ç½®å·²ä¿å­˜ã€‚")
    
    # 10. è¯„ä¼°è®­ç»ƒå¥½çš„Agent
    print("\n--- 10. è¯„ä¼°è®­ç»ƒå¥½çš„ Agent ---")
    
    # åœ¨å›ºå®šå‚æ•°ä¸‹è¯„ä¼°ï¼ˆå‚æ•°å·²åœ¨å‰é¢è®¾ç½®ï¼‰
    print("ä½¿ç”¨å›ºå®šæµ‹è¯•å‚æ•°ï¼ˆä¸MATLABç‰ˆæœ¬ä¸€è‡´ï¼‰:")
    for key, value in M_eval.items():
        print(f"  {key} = {value:.3f}")
    
    # å¯¹æ¯”ä¸åŒæŠ˜ç°ç‡çš„è¯„ä¼°ç»“æœ
    print("\nğŸ”§ æŠ˜ç°ç‡å¯¹æ¯”è¯„ä¼°:")
    
    # æ— æŠ˜ç°è¯„ä¼°ï¼ˆSB3é»˜è®¤ï¼‰
    mean_reward_no_discount, std_reward_no_discount = evaluate_policy(
        model, eval_env, n_eval_episodes=100, deterministic=True
    )
    
    # æ­£ç¡®çš„æŠ˜ç°è¯„ä¼°ï¼ˆÎ³=0.97ï¼‰
    mean_reward_discounted, std_reward_discounted = evaluate_policy_with_discount(
        model, eval_env, n_eval_episodes=100, deterministic=True, gamma=0.97
    )
    
    # ğŸ†• æ–°å¢ï¼šç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿè¯„ä¼°ï¼ˆä¸compare_rl_and_vfi.pyå®Œå…¨ç›¸åŒï¼‰
    mean_reward_lifecycle, std_reward_lifecycle, lifecycle_results = evaluate_policy_lifecycle_simulation(
        model, cS, paramS_for_rl, rng_M, n_sim=100, deterministic=True, gamma=0.97, random_seed=42
    )
    
    print(f"âŒ æ— æŠ˜ç°è¯„ä¼° (Î³=1.0): {mean_reward_no_discount:.2f} Â± {std_reward_no_discount:.2f}")
    print(f"âœ… æ­£ç¡®æŠ˜ç°è¯„ä¼°ï¼Œæœ‰éšæœºå®è§‚å˜é‡ (Î³=0.97): {mean_reward_discounted:.2f} Â± {std_reward_discounted:.2f}")
    print(f"ğŸ§¬ ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿè¯„ä¼°ï¼Œæ— éšæœºå®è§‚å˜é‡ (Î³=0.97): {mean_reward_lifecycle:.2f} Â± {std_reward_lifecycle:.2f}")
    print(f"ğŸ† è®­ç»ƒè¿‡ç¨‹æœ€ä½³ç»“æœ: {eval_callback.best_mean_reward:.2f} (timestep {getattr(eval_callback, 'best_timestep', 'N/A')})")
    print(f"ğŸ“Š æœ€ç»ˆ vs æœ€ä½³å·®å¼‚: {mean_reward_discounted - eval_callback.best_mean_reward:.2f}")
    print(f"ğŸ’¡ ä¸VFIæ¯”è¾ƒåº”ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿç»“æœ: {mean_reward_lifecycle:.2f}")
    
    print(f"\nğŸ“ˆ è¯„ä¼°å®Œæˆã€‚è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¯„ä¼°å·²ä½¿ç”¨æ­£ç¡®çš„æŠ˜ç°å› å­ (Î³=0.97)")
    
    # TensorBoardä½¿ç”¨æŒ‡å—
    print(f"\nğŸ“Š TensorBoard ç›‘æ§æŒ‡å—:")
    print(f"  å¯åŠ¨å‘½ä»¤: tensorboard --logdir=py/tensorboard_logs_sbx/ --port=6006")
    print(f"  è®¿é—®åœ°å€: http://localhost:6006/")
    print(f"  é‡è¦æŒ‡æ ‡:")
    print(f"    - train/actor_loss: Actorç½‘ç»œæŸå¤±")
    print(f"    - train/critic_loss: Criticç½‘ç»œæŸå¤±") 
    print(f"    - eval/mean_reward: è¯„ä¼°å¥–åŠ±ï¼ˆå¸¦æŠ˜ç°ï¼‰")
    print(f"    - eval/mean_ep_length: å¹³å‡å›åˆé•¿åº¦")
    print(f"    - train/ent_coef: ç†µç³»æ•°å˜åŒ–")
    
    # æ›´æ–°é…ç½®ä¿¡æ¯
    config['eval_gamma'] = 0.97
    config['mean_reward_no_discount'] = mean_reward_no_discount
    config['mean_reward_discounted'] = mean_reward_discounted
    config['mean_reward_lifecycle'] = mean_reward_lifecycle  # ğŸ†• æ–°å¢ç”Ÿå‘½å‘¨æœŸè¯„ä¼°ç»“æœ
    config['std_reward_lifecycle'] = std_reward_lifecycle
    config['lifecycle_results'] = lifecycle_results  # ğŸ†• ä¿å­˜å®Œæ•´çš„ç”Ÿå‘½å‘¨æœŸç»“æœ
    config['best_training_reward'] = eval_callback.best_mean_reward
    config['best_training_timestep'] = getattr(eval_callback, 'best_timestep', None)
    config['tensorboard_log_path'] = './py/tensorboard_logs_sbx/'
    
    # 11. ç»˜åˆ¶è®­ç»ƒç»Ÿè®¡
    print("\n--- 11. ç»˜åˆ¶è®­ç»ƒç»Ÿè®¡ ---")
    try:
        plot_training_stats('./py/logs_sbx/', './py/training_stats_sbx.png')
    except Exception as e:
        print(f"ç»˜å›¾å¤±è´¥: {e}")
    
    print("SBX SAC Agent è®­ç»ƒå’Œå¤„ç†æ¡†æ¶å®Œæˆã€‚")
    
    # æ·»åŠ ONNXå¯¼å‡ºä¿¡æ¯
    if 'onnx_export' in config:
        print("")
        print("ğŸ”„ ONNXæ¨¡å‹å¯¼å‡ºä¿¡æ¯:")
        print(f"  âœ… æœ€ç»ˆæ¨¡å‹ONNX: {config['onnx_export']['final_model']}")
        print(f"  âœ… æœ€ç»ˆæ¨¡å‹å‚æ•°: {config['onnx_export']['final_model_params']}")
        print(f"  âœ… æœ€ä½³æ¨¡å‹ONNX: {config['onnx_export']['best_model']}")
        print(f"  âœ… æœ€ä½³æ¨¡å‹å‚æ•°: {config['onnx_export']['best_model_params']}")
        print(f"  ğŸ’¡ ONNXæ¨¡å‹å’Œå‚æ•°æ–‡ä»¶å¯ç›´æ¥åœ¨MATLABä¸­ä½¿ç”¨")
    
    print("="*60)
    
    return model, config, M_eval

def plot_training_stats(log_path: str, save_path: str):
    """ç»˜åˆ¶è®­ç»ƒç»Ÿè®¡å›¾"""
    
    # è¯»å–è®­ç»ƒæ—¥å¿—
    log_file = os.path.join(log_path, 'evaluations.npz')
    if os.path.exists(log_file):
        data = np.load(log_file)
        timesteps = data['timesteps']
        results = data['results']
        
        plt.figure(figsize=(12, 4))
        
        # è¯„ä¼°å¥–åŠ±å›¾
        plt.subplot(1, 2, 1)
        mean_rewards = np.mean(results, axis=1)
        std_rewards = np.std(results, axis=1)
        plt.plot(timesteps, mean_rewards, 'b-', label='Mean Reward', linewidth=2)
        plt.fill_between(timesteps, mean_rewards - std_rewards, 
                        mean_rewards + std_rewards, alpha=0.3)
        plt.xlabel('Timesteps')
        plt.ylabel('Reward')
        plt.title('SBX SAC Training Progress: Evaluation Rewards')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å¥–åŠ±åˆ†å¸ƒå›¾
        plt.subplot(1, 2, 2)
        plt.boxplot(results.T, positions=timesteps[::max(1, len(timesteps)//10)])
        plt.xlabel('Timesteps')
        plt.ylabel('Reward')
        plt.title('Reward Distribution Over Training')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"è®­ç»ƒç»Ÿè®¡å›¾å·²ä¿å­˜åˆ° {save_path}")
    else:
        print(f"æœªæ‰¾åˆ°è®­ç»ƒæ—¥å¿—æ–‡ä»¶: {log_file}")

if __name__ == "__main__":
    # è¿è¡Œè®­ç»ƒ
    model, config, M_eval = main()
    
    # è¾“å‡ºæœ€ç»ˆç»“æœæ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ¯ SBX SAC è®­ç»ƒå®Œæˆæ‘˜è¦")
    print("="*60)
    print(f"ç®—æ³•: {config['algorithm']}")
    print(f"JAXè®¾å¤‡: {config['jax_devices']}")
    print(f"è®­ç»ƒæ—¶é—´: {config['training_time']:.2f} ç§’")
    print(f"ç½‘ç»œæ¶æ„: {config['model_kwargs']['policy_kwargs']['net_arch']}")
    print(f"è®­ç»ƒæŠ˜ç°å› å­: {config['model_kwargs']['gamma']}")
    print(f"è¯„ä¼°æŠ˜ç°å› å­: {config['eval_gamma']}")
    print(f"æ¨¡å‹ä¿å­˜è·¯å¾„: ./py/final_sac_agent_olg_sbx.zip")
    print("")
    print("ğŸ† è®­ç»ƒç»“æœæ‘˜è¦:")
    print(f"  æœ€ä½³è®­ç»ƒç»“æœ: {config['best_training_reward']:.2f} (timestep {config['best_training_timestep']})")
    print(f"  æœ€ç»ˆè¯„ä¼°ç»“æœ: {config['mean_reward_discounted']:.2f}")
    print(f"  ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿç»“æœ: {config['mean_reward_lifecycle']:.2f} Â± {config['std_reward_lifecycle']:.2f}")
    print(f"  æ€§èƒ½é€€åŒ–: {config['mean_reward_discounted'] - config['best_training_reward']:.2f}")
    print("")
    print("ğŸ”§ æŠ˜ç°ç‡ä¸€è‡´æ€§ç¡®è®¤:")
    print(f"  âœ… è®­ç»ƒæ—¶Î³ = {config['model_kwargs']['gamma']}")
    print(f"  âœ… è¯„ä¼°æ—¶Î³ = {config['eval_gamma']}")
    print(f"  âœ… ä¸VFIæ¯”è¾ƒæ—¶åº”ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿç»“æœ: {config['mean_reward_lifecycle']:.2f}")
    print(f"  ğŸ¯ ç”Ÿå‘½å‘¨æœŸè¯„ä¼°ä¸compare_rl_and_vfi.pyå®Œå…¨ä¸€è‡´")
    
    # æ·»åŠ ONNXå¯¼å‡ºä¿¡æ¯
    if 'onnx_export' in config:
        print("")
        print("ğŸ”„ ONNXæ¨¡å‹å¯¼å‡ºä¿¡æ¯:")
        print(f"  âœ… æœ€ç»ˆæ¨¡å‹ONNX: {config['onnx_export']['final_model']}")
        print(f"  âœ… æœ€ç»ˆæ¨¡å‹å‚æ•°: {config['onnx_export']['final_model_params']}")
        print(f"  âœ… æœ€ä½³æ¨¡å‹ONNX: {config['onnx_export']['best_model']}")
        print(f"  âœ… æœ€ä½³æ¨¡å‹å‚æ•°: {config['onnx_export']['best_model_params']}")
        print(f"  ğŸ’¡ ONNXæ¨¡å‹å’Œå‚æ•°æ–‡ä»¶å¯ç›´æ¥åœ¨MATLABä¸­ä½¿ç”¨")
    
    print("="*60) 
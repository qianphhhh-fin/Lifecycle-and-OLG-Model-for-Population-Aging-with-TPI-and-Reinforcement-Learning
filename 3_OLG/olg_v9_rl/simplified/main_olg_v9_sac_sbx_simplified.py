"""
OLG Model V9 SAC Training Script - Simplified Version with Fixed Macro Parameters
ç®€åŒ–ç‰ˆSACè®­ç»ƒè„šæœ¬ - ä½¿ç”¨å›ºå®šå®è§‚å‚æ•°

ğŸ¯ æ ¸å¿ƒç®€åŒ–ï¼š
- å®è§‚å‚æ•°åœ¨ç¯å¢ƒåˆå§‹åŒ–æ—¶å›ºå®š
- RLçŠ¶æ€å˜é‡ä¸VFIåŸºæœ¬ç›¸åŒï¼š(k, k_pps, age, Îµ) - 4ç»´
- ä¿æŒç´¯ç§¯å­˜æ´»æ¦‚ç‡æ–¹æ³•çš„ç†è®ºç­‰ä»·æ€§
- æ›´å…¬å¹³åœ°æ¯”è¾ƒRLå’ŒVFIæ€§èƒ½

ğŸ® åŠ¨ä½œç©ºé—´è®¾è®¡ï¼š
- 2ç»´è¿ç»­åŠ¨ä½œï¼š[PPSç¼´è´¹æ¯”ä¾‹, æ¶ˆè´¹æ¯”ä¾‹]
- å†³ç­–é¡ºåºï¼šå…ˆPPSç¼´è´¹ â†’ å†æ¶ˆè´¹ â†’ æœ€åå‚¨è“„ï¼ˆè‡ªåŠ¨ï¼‰
- PPSç¼´è´¹æ¯”ä¾‹ï¼š[0, pps_max_contrib_frac]
- æ¶ˆè´¹æ¯”ä¾‹ï¼š[0, 1.0]ï¼Œä½œç”¨äºå¯ç”¨äºæ¶ˆè´¹çš„èµ„æº

ğŸ’¡ å†³ç­–é€»è¾‘ï¼š
1. æ ¹æ®PPSç¼´è´¹æ¯”ä¾‹ç¡®å®šPPSç¼´è´¹
2. æ ¹æ®æ¶ˆè´¹æ¯”ä¾‹ç¡®å®šæ¶ˆè´¹æ”¯å‡ºï¼ˆåŸºç¡€æ¶ˆè´¹ + æ¯”ä¾‹ Ã— è¶…é¢èµ„æºï¼‰
3. å‰©ä½™èµ„æºè‡ªåŠ¨ç”¨äºå‚¨è“„

ä¸»è¦å˜åŒ–ï¼š
1. ç¯å¢ƒä½¿ç”¨å›ºå®šå®è§‚å‚æ•°ï¼Œä¸å†ä½œä¸ºçŠ¶æ€å˜é‡
2. è§‚æµ‹ç©ºé—´ä»10ç»´é™åˆ°4ç»´
3. åŠ¨ä½œç©ºé—´è¯­ä¹‰æ”¹ä¸º[PPSç¼´è´¹æ¯”ä¾‹, æ¶ˆè´¹æ¯”ä¾‹]
4. å…¶ä»–è®­ç»ƒé€»è¾‘ä¿æŒä¸å˜
5. ä¿æŒç´¯ç§¯å­˜æ´»æ¦‚ç‡æ–¹æ³•
"""

import numpy as np
import jax
import pickle
import time
import os
from typing import Dict, Any, Tuple
from sbx import SAC  # ä½¿ç”¨æ ‡å‡†SBX SAC
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold
from stable_baselines3.common.monitor import Monitor
import matplotlib.pyplot as plt

# å¯¼å…¥ç®€åŒ–ç‰ˆå·¥å…·ç±»
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from main_olg_v9_utils import OLG_V9_Utils
from simplified.main_olg_v9_utils_simplified import OLGEnvV9SACSimplified, OLGUtilsSimplified

def evaluate_policy_lifecycle_simulation_simplified(model, cS, paramS_for_rl, M_fixed, 
                                                   n_sim=50, deterministic=True, 
                                                   gamma=0.97, random_seed=42, verbose=True,
                                                   eIdxM_annual_input=None):
    """
    ç®€åŒ–ç‰ˆç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿè¯„ä¼°
    
    Args:
        model: RLæ¨¡å‹
        cS: æ¨¡å‹å‚æ•°
        paramS_for_rl: RLä¸“ç”¨å‚æ•°
        M_fixed: å›ºå®šå®è§‚å‚æ•°
        n_sim: æ¨¡æ‹Ÿä¸ªä½“æ•°é‡
        deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
        gamma: æŠ˜ç°å› å­ï¼ˆåº”è¯¥ç­‰äºcS.betaï¼‰
        random_seed: éšæœºç§å­
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        eIdxM_annual_input: å¯é€‰çš„é¢„ç”Ÿæˆæ•ˆç‡å†²å‡»åºåˆ—
        
    Returns:
        mean_reward: å¹³å‡ç”Ÿå‘½å‘¨æœŸæŠ˜ç°æ•ˆç”¨
        std_reward: æ ‡å‡†å·®
        lifecycle_results: è¯¦ç»†çš„ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿç»“æœ
    """
    
    sim_start_time = time.time()

    if verbose:
        print(f"\nğŸ§¬ ç®€åŒ–ç‰ˆç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿè¯„ä¼°")
        print(f"   (n_sim={n_sim}, seed={random_seed})")
        print("ğŸ“Š å›ºå®šå®è§‚å‚æ•°:")
        for key, value in M_fixed.items():
            print(f"     {key} = {value:.3f}")

    # è®¾ç½®éšæœºç§å­å¹¶ç”Ÿæˆæ•ˆç‡å†²å‡»è·¯å¾„
    np.random.seed(random_seed)
    aD_orig = int(cS.aD_orig)
    nw = int(cS.nw)
    leProb1V = np.array(paramS_for_rl['leProb1V']).flatten()
    leTrProbM = np.array(paramS_for_rl['leTrProbM'])
    
    if eIdxM_annual_input is None:
        if verbose: print("ğŸ”„ ç”Ÿæˆå¹´åº¦æ•ˆç‡å†²å‡»è·¯å¾„...")
        random_numbers = np.random.rand(n_sim, aD_orig)
        eIdxM_annual = OLG_V9_Utils.MarkovChainSimulation(
            n_sim, aD_orig, leProb1V, leTrProbM, random_numbers
        )
    else:
        if verbose: print("ğŸ”„ ä½¿ç”¨é¢„ç”Ÿæˆçš„å¹´åº¦æ•ˆç‡å†²å‡»è·¯å¾„...")
        eIdxM_annual = eIdxM_annual_input
        if eIdxM_annual.shape[0] != n_sim:
            print(f"âš ï¸ è­¦å‘Šï¼šé¢„ç”Ÿæˆçš„æ•ˆç‡å†²å‡»ä¸ªä½“æ•° ({eIdxM_annual.shape[0]}) ä¸ n_sim ({n_sim}) ä¸ç¬¦ã€‚å°†ä½¿ç”¨é¢„ç”Ÿæˆè·¯å¾„çš„ä¸ªä½“æ•°ã€‚")
            n_sim = eIdxM_annual.shape[0]

    # è°ƒç”¨ç®€åŒ–ç‰ˆæ ¸å¿ƒæ¨¡æ‹Ÿå‡½æ•°
    if verbose: print("ğŸš€ è°ƒç”¨ç®€åŒ–ç‰ˆç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿ...")
    k_path_rl_group, kpps_path_rl_group, c_path_rl_group, cpps_path_rl_group = OLGUtilsSimplified.HHSimulation_olgm_rl_simplified(
        model, cS, paramS_for_rl, M_fixed, eIdxM_annual
    )
    if verbose: print("âœ… æ ¸å¿ƒæ¨¡æ‹Ÿå®Œæˆã€‚")

    aD_new = int(cS.aD_new)
    
    # åˆå§‹åŒ–å¹´åº¦è·¯å¾„çŸ©é˜µ
    k_path_rl = np.zeros((n_sim, aD_orig))
    kpps_path_rl = np.zeros((n_sim, aD_orig))
    c_path_rl = np.zeros((n_sim, aD_orig))
    cpps_path_rl = np.zeros((n_sim, aD_orig))
    

    # æ ¹æ®è¿”å›çš„æ¶ˆè´¹è·¯å¾„è®¡ç®—ç”Ÿå‘½å‘¨æœŸæ•ˆç”¨
    if verbose: print("ğŸ“Š è®¡ç®—ç”Ÿå‘½å‘¨æœŸæ•ˆç”¨...")
    lifetime_utility_rl = np.zeros(n_sim)
    beta = float(cS.beta)
    for i in range(n_sim):
        utility_sum = 0
        for a in range(aD_orig):
            c = c_path_rl[i, a]
            _, u = OLG_V9_Utils.CES_utility(c, cS.sigma, cS)
            utility_sum += (beta ** a) * u
        lifetime_utility_rl[i] = utility_sum

    mean_utility_rl = np.mean(lifetime_utility_rl)
    std_utility_rl = np.std(lifetime_utility_rl)
    
    sim_time = time.time() - sim_start_time

    if verbose:
        print(f"âœ… ç®€åŒ–ç‰ˆç”Ÿå‘½å‘¨æœŸè¯„ä¼°å®Œæˆï¼Œè€—æ—¶: {sim_time:.2f} ç§’")
        print(f"ğŸ“Š RLç”Ÿå‘½å‘¨æœŸæ•ˆç”¨: {mean_utility_rl:.4f} Â± {std_utility_rl:.4f}")
    
    # æ•´ç†å¹¶è¿”å›ç»“æœ
    ageToGroupMap = np.zeros(cS.aD_orig, dtype=int)
    for i_group, indices in enumerate(cS.physAgeMap):
        if indices:
            ageToGroupMap[np.array(indices)] = i_group

    lifecycle_results = {
        'success': True,
        'n_sim': n_sim,
        'random_seed': random_seed,
        'M_fixed': M_fixed,
        'simulation_time': sim_time,
        'mean_utility_rl': mean_utility_rl,
        'std_utility_rl': std_utility_rl,
        'lifetime_utility_rl': lifetime_utility_rl,
        'k_path_rl': k_path_rl,
        'kpps_path_rl': kpps_path_rl,
        'c_path_rl': c_path_rl,
        'cpps_path_rl': cpps_path_rl,
        'eIdxM_annual': eIdxM_annual,
        'aD_orig': aD_orig,
        'aD_new': int(cS.aD_new),
        'ageToGroupMap': ageToGroupMap,
        'beta': beta,
        'gamma': gamma,
        'time_scale': 'annual_matlab_consistent',
        'matlab_compatible': True,
        'vfi_equivalent': True,
        'simplified_version': True,
        'state_space_dim': 4  # ç®€åŒ–ç‰ˆç‰¹å¾
    }
    
    return mean_utility_rl, std_utility_rl, lifecycle_results

class EvalCallbackSimplified(EvalCallback):
    """
    ç®€åŒ–ç‰ˆè¯„ä¼°å›è°ƒ
    """
    
    def __init__(self, eval_env, cS, paramS_for_rl, M_fixed, gamma=0.97, 
                 use_lifecycle_simulation=True, **kwargs):
        """
        Args:
            eval_env: è¯„ä¼°ç¯å¢ƒ
            cS: æ¨¡å‹å‚æ•°
            paramS_for_rl: RLä¸“ç”¨å‚æ•°
            M_fixed: å›ºå®šå®è§‚å‚æ•°
            gamma: æŠ˜ç°å› å­
            use_lifecycle_simulation: æ˜¯å¦ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿ
            **kwargs: å…¶ä»–EvalCallbackå‚æ•°
        """
        super().__init__(eval_env, **kwargs)
        self.gamma = gamma
        self.cS = cS
        self.paramS_for_rl = paramS_for_rl
        self.M_fixed = M_fixed
        self.use_lifecycle_simulation = use_lifecycle_simulation
        
        # ç”Ÿæˆç»Ÿä¸€çš„æ•ˆç‡å†²å‡»è·¯å¾„
        if use_lifecycle_simulation:
            self._generate_unified_efficiency_shocks()
        
        print(f"ğŸ”§ ç®€åŒ–ç‰ˆè¯„ä¼°å›è°ƒ (çŠ¶æ€ç©ºé—´4ç»´, Î³={gamma})")
        print("ğŸ¯ åŸºäºç´¯ç§¯å­˜æ´»æ¦‚ç‡æ–¹æ³•ï¼Œä¸VFIç†è®ºå®Œå…¨ç­‰ä»·")

    def _generate_unified_efficiency_shocks(self):
        """ç”Ÿæˆç»Ÿä¸€çš„æ•ˆç‡å†²å‡»è·¯å¾„"""
        print("ğŸ² ç”Ÿæˆç»Ÿä¸€æ•ˆç‡å†²å‡»åºåˆ—ï¼ˆç®€åŒ–ç‰ˆï¼‰...")
        
        np.random.seed(42)
        n_sim_target = max(self.n_eval_episodes, 100)
        aD_orig = int(self.cS.aD_orig)
        
        leProb1V = np.array(self.paramS_for_rl['leProb1V']).flatten()
        leTrProbM = np.array(self.paramS_for_rl['leTrProbM'])
        
        random_numbers = np.random.rand(n_sim_target, aD_orig)
        # self.eIdxM_unified = OLG_V9_Utils.MarkovChainSimulation_(
        #     n_sim_target, aD_orig, leProb1V, leTrProbM, random_numbers
        # )

        self.eIdxM_unified = OLG_V9_Utils.LaborEndowSimulation_olgm_AgeGroup(self.cS, self.paramS_for_rl)
        
        print(f"âœ… ç»Ÿä¸€æ•ˆç‡å†²å‡»çŸ©é˜µç”Ÿæˆå®Œæˆ: {self.eIdxM_unified.shape}")

    def _on_step(self) -> bool:
        """é‡å†™_on_stepæ–¹æ³•"""
        continue_training = True

        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:
            episode_rewards, episode_lengths = self._evaluate_with_discount()
            
            if self.log_path is not None:
                self.evaluations_timesteps.append(self.num_timesteps)
                self.evaluations_results.append(episode_rewards)
                self.evaluations_length.append(episode_lengths)

                kwargs = {}
                if len(self._is_success_buffer) > 0:
                    self.evaluations_successes.append(self._is_success_buffer)
                    kwargs = dict(successes=self.evaluations_successes)

                np.savez(
                    self.log_path,
                    timesteps=self.evaluations_timesteps,
                    results=self.evaluations_results,
                    ep_lengths=self.evaluations_length,
                    **kwargs,
                )

            mean_reward, std_reward = np.mean(episode_rewards), np.std(episode_rewards)
            mean_ep_length, std_ep_length = np.mean(episode_lengths), np.std(episode_lengths)
            self.last_mean_reward = mean_reward

            if self.verbose >= 1:
                print(f"ğŸ“Š Eval num_timesteps={self.num_timesteps}")
                print(f"   Current reward: {mean_reward:.2f} +/- {std_reward:.2f} (ç®€åŒ–ç‰ˆ)")
                print(f"   Episode length: {mean_ep_length:.2f} +/- {std_ep_length:.2f}")
                print(f"ğŸ† Best reward: {self.best_mean_reward:.2f} (at timestep {getattr(self, 'best_timestep', 'N/A')})")

            self.logger.record("eval/mean_reward", float(mean_reward))
            self.logger.record("eval/mean_ep_length", mean_ep_length)

            if len(self._is_success_buffer) > 0:
                success_rate = np.mean(self._is_success_buffer)
                if self.verbose >= 1:
                    print(f"Success rate: {100 * success_rate:.2f}%")
                self.logger.record("eval/success_rate", success_rate)

            self.logger.record("time/total_timesteps", self.num_timesteps, exclude="tensorboard")
            self.logger.dump(self.num_timesteps)

            if mean_reward > self.best_mean_reward:
                if self.verbose >= 1:
                    print("ğŸ† New best mean reward!")
                if self.best_model_save_path is not None:
                    self.model.save(os.path.join(self.best_model_save_path, "best_model"))
                self.best_mean_reward = mean_reward
                self.best_timestep = self.num_timesteps

                if self.callback_on_new_best is not None:
                    continue_training = self.callback_on_new_best.on_step()

        return continue_training
    
    def _evaluate_with_discount(self):
        """ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿæˆ–æ ‡å‡†æŠ˜ç°è¿›è¡Œè¯„ä¼°"""
        if self.use_lifecycle_simulation:
            return self._evaluate_with_lifecycle_simulation()
        else:
            return self._evaluate_with_traditional_discount()
    
    def _evaluate_with_lifecycle_simulation(self):
        """ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿè¿›è¡Œè¯„ä¼°"""
        eval_random_seed = 42
        n_sim_actual = max(self.n_eval_episodes, 100)
        eIdxM_input = self.eIdxM_unified[:n_sim_actual, :] if hasattr(self, 'eIdxM_unified') else None
        
        mean_reward, std_reward, lifecycle_results = evaluate_policy_lifecycle_simulation_simplified(
            self.model, self.cS, self.paramS_for_rl, self.M_fixed,
            n_sim=n_sim_actual, 
            deterministic=self.deterministic,
            gamma=self.gamma,
            random_seed=eval_random_seed,
            verbose=False,
            eIdxM_annual_input=eIdxM_input
        )
        
        episode_rewards = [mean_reward] * self.n_eval_episodes
        episode_lengths = [lifecycle_results['aD_orig']] * self.n_eval_episodes
        
        if self.verbose >= 1:
            print(f"   ğŸ§¬ ç®€åŒ–ç‰ˆç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿ (n_sim={lifecycle_results['n_sim']}, seed={eval_random_seed})")
            print(f"   ğŸ“Š æ¨¡æ‹Ÿç»“æœ: {mean_reward:.4f} Â± {std_reward:.4f}")
        
        return episode_rewards, episode_lengths
    
    def _evaluate_with_traditional_discount(self):
        """ä¼ ç»Ÿçš„æŠ˜ç°è¯„ä¼°æ–¹æ³•ï¼ˆå¤‡ç”¨ï¼‰"""
        episode_rewards = []
        episode_lengths = []
        
        for i in range(self.n_eval_episodes):
            reset_result = self.eval_env.reset()
            if isinstance(reset_result, tuple):
                obs, _ = reset_result
            else:
                obs = reset_result
                
            done = False
            episode_reward = 0.0
            episode_length = 0
            discount_factor = 1.0
            
            while not done:
                action, _ = self.model.predict(obs, deterministic=self.deterministic)
                
                step_result = self.eval_env.step(action)
                if len(step_result) == 5:
                    obs, reward, terminated, truncated, info = step_result
                    done = terminated or truncated
                elif len(step_result) == 4:
                    obs, reward, done, info = step_result
                else:
                    raise ValueError(f"Unexpected step() return format: {len(step_result)} values")
                
                episode_reward += discount_factor * reward
                discount_factor *= self.gamma
                episode_length += 1
                
                if self.render:
                    self.eval_env.render()
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
        
        return episode_rewards, episode_lengths

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("=== OLG æ¨¡å‹ V9 - SAC æ™ºèƒ½ä½“è®­ç»ƒ (ç®€åŒ–ç‰ˆ - å›ºå®šå®è§‚å‚æ•°) ===")
    print("    ğŸ¯ çŠ¶æ€ç©ºé—´ï¼š(k, k_pps, age, Îµ) - 4ç»´ï¼Œä¸VFIåŸºæœ¬ç›¸åŒ")
    print("    ğŸ”§ å®è§‚å‚æ•°å›ºå®šï¼Œä¸ä½œä¸ºçŠ¶æ€å˜é‡")
    print("    ğŸ’¡ ç´¯ç§¯å­˜æ´»æ¦‚ç‡æ–¹æ³•ä¿æŒä¸å˜")
    print(f"    (JAXè®¾å¤‡: {jax.devices()})")
    
    # 1. åˆå§‹åŒ–å‚æ•°
    print("\n--- 1. åˆå§‹åŒ–å‚æ•° ---")
    cS = OLG_V9_Utils.ParameterValues_HuggettStyle()
    
    # è®¡ç®—RLç›¸å…³å‚æ•°
    paramS_for_rl = {}
    if (not hasattr(cS, 'leGridV') or not hasattr(cS, 'leTrProbM') or not hasattr(cS, 'leProb1V')):
        (paramS_for_rl['leLogGridV'], 
         paramS_for_rl['leTrProbM'], 
         paramS_for_rl['leProb1V']) = OLG_V9_Utils.EarningProcess_olgm(cS)
        paramS_for_rl['leGridV'] = np.exp(paramS_for_rl['leLogGridV'])
        cS.leGridV = paramS_for_rl['leGridV']
        cS.leTrProbM = paramS_for_rl['leTrProbM']
        cS.leProb1V = paramS_for_rl['leProb1V']
    else:
        paramS_for_rl['leGridV'] = cS.leGridV
        paramS_for_rl['leTrProbM'] = cS.leTrProbM
        paramS_for_rl['leProb1V'] = cS.leProb1V
    
    paramS_for_rl['ageEffV_new'] = cS.ageEffV_new
    
    # 2. å®šä¹‰å›ºå®šå®è§‚å‚æ•°
    print("\n--- 2. å®šä¹‰å›ºå®šå®è§‚å‚æ•° ---")
    M_fixed = {
        'R_k_net_factor': 1.03,
        'w_gross': 2.0,
        'TR_total': 0.1,
        'b_payg_avg_retiree': 0.4,
        'tau_l': 0.15,
        'theta_payg_actual': 0.12
    }
    
    print("ğŸ¯ å›ºå®šå®è§‚å‚æ•°ï¼ˆä¸VFIæµ‹è¯•å‚æ•°ä¸€è‡´ï¼‰:")
    for key, value in M_fixed.items():
        print(f"  {key} = {value:.3f}")
    
    # 3. åˆ›å»ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
    print("\n--- 3. åˆ›å»ºç®€åŒ–ç‰ˆå¼ºåŒ–å­¦ä¹ ç¯å¢ƒ ---")
    # è®­ç»ƒç¯å¢ƒï¼šä½¿ç”¨è®­ç»ƒæ¨¡å¼
    env = OLGEnvV9SACSimplified(cS, paramS_for_rl, M_fixed, training_mode=True)
    env = Monitor(env)
    
    print(f"è§‚æµ‹ç©ºé—´: {env.observation_space} (4ç»´)")
    print(f"åŠ¨ä½œç©ºé—´: {env.action_space} ([PPSç¼´è´¹æ¯”ä¾‹, æ¶ˆè´¹æ¯”ä¾‹])")
    print("ğŸ¯ ç®€åŒ–ç‰ˆè®­ç»ƒç¯å¢ƒå·²åˆ›å»ºï¼ˆå›ºå®šå®è§‚å‚æ•°ï¼‰")
    print("ğŸ’¡ åŠ¨ä½œç©ºé—´ï¼šå…ˆPPSç¼´è´¹ï¼Œå†æ¶ˆè´¹ï¼Œæœ€åè‡ªåŠ¨å‚¨è“„")
    
    # 4. åˆ›å»ºSBX SAC Agent
    print("\n--- 4. åˆ›å»º SBX SAC Agent ---")
    
    model_kwargs = {
        'policy': 'MlpPolicy',
        'env': env,
        'learning_rate': 3e-5,
        'buffer_size': int(1e6),
        'batch_size': 256,
        'tau': 5e-3,
        'gamma': 0.97,
        'ent_coef': 'auto',
        'gradient_steps': 1,
        'policy_kwargs': {
            'net_arch': [256, 256],  # ç®€åŒ–ç½‘ç»œæ¶æ„
        },
        'verbose': 1,
        'learning_starts': 5000,
        'seed': 42,
        'tensorboard_log': './simplified/tensorboard_logs_simplified/'
    }
    
    print("åˆ›å»ºç®€åŒ–ç‰ˆSBX SACæ¨¡å‹...")
    model = SAC(**model_kwargs)
    print("ç®€åŒ–ç‰ˆSBX SAC Agentå·²åˆ›å»ºã€‚")
    print(f"ç½‘ç»œæ¶æ„: {model_kwargs['policy_kwargs']}")
    
    # 5. è®¾ç½®è®­ç»ƒå‚æ•°
    print("\n--- 5. è®¾ç½®è®­ç»ƒå‚æ•° ---")
    max_steps_per_episode = cS.aD_new
    total_timesteps = 300_000  # ç®€åŒ–ç‰ˆä½¿ç”¨è¾ƒå°‘çš„è®­ç»ƒæ­¥æ•°
    stop_training_value = -20
    eval_freq = 1_000
    n_eval_episodes = 100
    
    print(f"æ¯å›åˆæœ€å¤§æ­¥æ•°: {max_steps_per_episode}")
    print(f"æ€»è®­ç»ƒæ­¥æ•°: {total_timesteps}")
    print(f"çŠ¶æ€ç©ºé—´ç»´åº¦: 4 (ç®€åŒ–ç‰ˆ)")
    
    # 6. æµ‹è¯•ç¯å¢ƒå’Œæ™ºèƒ½ä½“åˆå§‹åŒ–
    print("\n--- 6. æµ‹è¯•ç¯å¢ƒå’Œæ™ºèƒ½ä½“åˆå§‹åŒ– ---")
    obs, _ = env.reset()
    print(f"ç¯å¢ƒé‡ç½®æˆåŠŸï¼Œè§‚å¯Ÿç»´åº¦: {obs.shape} (4ç»´)")
    
    action, _ = model.predict(obs, deterministic=False)
    print(f"æ™ºèƒ½ä½“åˆå§‹åŠ¨ä½œç”ŸæˆæˆåŠŸï¼ŒåŠ¨ä½œç»´åº¦: {action.shape}")
    print(f"åŠ¨ä½œå€¼: [PPSç¼´è´¹æ¯”ä¾‹={action[0]:.4f}, æ¶ˆè´¹æ¯”ä¾‹={action[1]:.4f}]")
    
    # 7. è®¾ç½®è¯„ä¼°å›è°ƒ
    print("\n--- 7. è®¾ç½®è¯„ä¼°å’Œå›è°ƒ ---")
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒï¼ˆè¯„ä¼°æ¨¡å¼ï¼‰
    eval_env_base = OLGEnvV9SACSimplified(cS, paramS_for_rl, M_fixed, training_mode=False)
    eval_env = Monitor(eval_env_base)
    
    stop_callback = StopTrainingOnRewardThreshold(
        reward_threshold=stop_training_value, 
        verbose=1
    )
    
    # è®¾ç½®è¯„ä¼°å›è°ƒ
    eval_callback = EvalCallbackSimplified(
        eval_env,
        cS,
        paramS_for_rl,
        M_fixed,
        gamma=0.97,
        use_lifecycle_simulation=True,
        best_model_save_path='./simplified/best_model_simplified/',
        log_path='./simplified/logs_simplified/',
        eval_freq=eval_freq,
        n_eval_episodes=n_eval_episodes,
        deterministic=True,
        render=False,
        callback_on_new_best=stop_callback,
        verbose=1
    )
    
    print("ğŸ§¬ ç®€åŒ–ç‰ˆç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿè¯„ä¼°å›è°ƒå·²è®¾ç½®ã€‚")
    
    # 8. å¼€å§‹è®­ç»ƒ
    print("\n--- 8. å¼€å§‹è®­ç»ƒ ---")
    print("ä½¿ç”¨ç®€åŒ–ç‰ˆSBX SACç®—æ³•è®­ç»ƒ...")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs('./simplified/best_model_simplified/', exist_ok=True)
    os.makedirs('./simplified/logs_simplified/', exist_ok=True)
    os.makedirs('./simplified/tensorboard_logs_simplified/', exist_ok=True)
    
    start_time = time.time()
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=eval_callback,
            log_interval=100,
            progress_bar=True
        )
        print("è®­ç»ƒå®Œæˆã€‚")
    except KeyboardInterrupt:
        print("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ã€‚")
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    training_time = time.time() - start_time
    print(f"è®­ç»ƒç”¨æ—¶: {training_time:.2f} ç§’")
    
    # 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\n--- 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---")
    model.save('./simplified/final_sac_agent_olg_simplified')

    # ä¿å­˜é…ç½®
    config = {
        'cS': cS,
        'paramS_for_rl': paramS_for_rl,
        'M_fixed': M_fixed,
        'model_kwargs': model_kwargs,
        'training_time': training_time,
        'algorithm': 'SBX_SAC_Simplified',
        'state_space_dim': 4,
        'simplified_version': True,
        'fixed_macro_params': True,
        'jax_devices': str(jax.devices())
    }

    with open('./simplified/training_config_simplified.pkl', 'wb') as f:
        pickle.dump(config, f)
    
    print("ç®€åŒ–ç‰ˆæ¨¡å‹å’Œé…ç½®å·²ä¿å­˜ã€‚")
    
    # 10. è¯„ä¼°è®­ç»ƒå¥½çš„Agent
    print("\n--- 10. è¯„ä¼°è®­ç»ƒå¥½çš„ Agent ---")
    
    print("ä½¿ç”¨å›ºå®šå®è§‚å‚æ•°:")
    for key, value in M_fixed.items():
        print(f"  {key} = {value:.3f}")
    
    # 1. æ— æŠ˜ç°è¯„ä¼°
    mean_reward_no_discount, std_reward_no_discount = evaluate_policy(
        model, eval_env, n_eval_episodes=100, deterministic=True
    )
    
    # 2. ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿè¯„ä¼°ï¼ˆç®€åŒ–ç‰ˆï¼‰
    eIdxM_annual_unified = None
    if hasattr(eval_callback, 'eIdxM_annual_unified'):
        eIdxM_annual_unified = eval_callback.eIdxM_annual_unified[:100, :]
        print("ğŸ¯ ä½¿ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆçš„ç»Ÿä¸€æ•ˆç‡å†²å‡»è·¯å¾„è¿›è¡Œæœ€ç»ˆè¯„ä¼°")
    
    mean_reward_lifecycle, std_reward_lifecycle, lifecycle_results = evaluate_policy_lifecycle_simulation_simplified(
        model, cS, paramS_for_rl, M_fixed, n_sim=100, deterministic=True, gamma=0.97, random_seed=42,
        eIdxM_annual_input=eIdxM_annual_unified
    )
    
    print(f"\nğŸ“Š ç®€åŒ–ç‰ˆè¯„ä¼°ç»“æœ:")
    print(f"âŒ æ— æŠ˜ç° (Î³=1.0): {mean_reward_no_discount:.2f} Â± {std_reward_no_discount:.2f}")
    print(f"ğŸ§¬ ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿ (ç®€åŒ–ç‰ˆ): {mean_reward_lifecycle:.2f} Â± {std_reward_lifecycle:.2f}")
    print(f"ğŸ† è®­ç»ƒè¿‡ç¨‹æœ€ä½³ç»“æœ: {eval_callback.best_mean_reward:.2f} (timestep {getattr(eval_callback, 'best_timestep', 'N/A')})")
    
    print(f"\nğŸ¯ ç®€åŒ–ç‰ˆç‰¹æ€§:")
    print(f"  ğŸ“Š çŠ¶æ€ç©ºé—´: (k, k_pps, age, Îµ) - 4ç»´ï¼Œä¸VFIåŸºæœ¬ç›¸åŒ")
    print(f"  ğŸ”§ å›ºå®šå®è§‚å‚æ•°: æ¶ˆé™¤å®è§‚ä¸ç¡®å®šæ€§")
    print(f"  ğŸ‹ï¸ è®­ç»ƒæ—¶: reward = u(c) * âˆ_{{i=1}}^{{t}} s(i) - ç´¯ç§¯å­˜æ´»æ¦‚ç‡åŠ æƒ")
    print(f"  ğŸ“Š è¯„ä¼°æ—¶: reward = u(c) - çº¯æ•ˆç”¨ï¼Œä¸VFIå¯æ¯”")
    print(f"  ğŸ’¡ ç†è®ºåŸºç¡€: æ•°å­¦ç­‰ä»·äºVFIçš„Bellmanæ–¹ç¨‹")
    
    print(f"\nğŸ” ç®€åŒ–ç‰ˆä¼˜åŠ¿:")
    print(f"  âœ… çŠ¶æ€ç©ºé—´ä¸VFIæ›´æ¥è¿‘ï¼Œæ›´å…¬å¹³çš„æ¯”è¾ƒ")
    print(f"  âœ… æ¶ˆé™¤å®è§‚å‚æ•°ä¸ç¡®å®šæ€§çš„å½±å“")
    print(f"  âœ… ä¿æŒç´¯ç§¯å­˜æ´»æ¦‚ç‡æ–¹æ³•çš„ç†è®ºç­‰ä»·æ€§")
    print(f"  ğŸ’¡ æ¨èè¯„ä¼°æ–¹å¼: ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿ ({mean_reward_lifecycle:.2f})")
    
    # æ›´æ–°é…ç½®ä¿¡æ¯
    config['mean_reward_no_discount'] = mean_reward_no_discount
    config['mean_reward_lifecycle'] = mean_reward_lifecycle
    config['std_reward_no_discount'] = std_reward_no_discount
    config['std_reward_lifecycle'] = std_reward_lifecycle
    config['lifecycle_results'] = lifecycle_results
    config['best_training_reward'] = eval_callback.best_mean_reward
    config['best_training_timestep'] = getattr(eval_callback, 'best_timestep', None)
    
    # é‡æ–°ä¿å­˜é…ç½®
    with open('./simplified/training_config_simplified.pkl', 'wb') as f:
        pickle.dump(config, f)
    
    print("ç®€åŒ–ç‰ˆSBX SAC Agent è®­ç»ƒå®Œæˆã€‚")
    print("="*60)
    
    return model, config, M_fixed

if __name__ == "__main__":
    # è¿è¡Œè®­ç»ƒ
    model, config, M_fixed = main()
    
    # è¾“å‡ºæœ€ç»ˆç»“æœæ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ¯ ç®€åŒ–ç‰ˆSBX SAC è®­ç»ƒå®Œæˆæ‘˜è¦")
    print("="*60)
    print(f"ç®—æ³•: {config['algorithm']}")
    print(f"çŠ¶æ€ç©ºé—´ç»´åº¦: {config['state_space_dim']} (ç®€åŒ–ç‰ˆ)")
    print(f"å›ºå®šå®è§‚å‚æ•°: {config['fixed_macro_params']}")
    print(f"è®­ç»ƒæ—¶é—´: {config['training_time']:.2f} ç§’")
    print(f"æ¨¡å‹ä¿å­˜è·¯å¾„: ./simplified/final_sac_agent_olg_simplified.zip")
    print("")
    print("ğŸ† è®­ç»ƒç»“æœæ‘˜è¦:")
    print(f"  æœ€ä½³è®­ç»ƒç»“æœ: {config['best_training_reward']:.2f} (timestep {config['best_training_timestep']})")
    print("")
    print("ğŸ¯ ç®€åŒ–ç‰ˆè¯„ä¼°:")
    print(f"  âŒ æ— æŠ˜ç°: {config['mean_reward_no_discount']:.2f} Â± {config['std_reward_no_discount']:.2f}")
    print(f"  ğŸ§¬ ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿ: {config['mean_reward_lifecycle']:.2f} Â± {config['std_reward_lifecycle']:.2f}")
    print("")
    print("ğŸ” ç®€åŒ–ç‰ˆç‰¹æ€§:")
    print(f"  âœ… çŠ¶æ€ç©ºé—´: (k, k_pps, age, Îµ) - ä¸VFIåŸºæœ¬ç›¸åŒ")
    print(f"  âœ… å›ºå®šå®è§‚å‚æ•°: æ¶ˆé™¤å®è§‚ä¸ç¡®å®šæ€§")
    print(f"  âœ… ç´¯ç§¯å­˜æ´»æ¦‚ç‡æ–¹æ³•: ç†è®ºä¸VFIå®Œå…¨ç­‰ä»·")
    print(f"  ğŸ’¡ æ›´å…¬å¹³çš„RL vs VFIæ¯”è¾ƒ")
    print("="*60) 
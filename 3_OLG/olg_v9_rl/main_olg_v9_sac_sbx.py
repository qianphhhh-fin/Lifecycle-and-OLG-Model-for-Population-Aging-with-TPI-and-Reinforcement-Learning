# --- å¼€å§‹æ–‡ä»¶ï¼štrain_sac_agent_olg_v9_sbx.py (ä¿®æ­£ç‰ˆ) ---

"""
OLG Model V9 SAC Training Script - SBX (Stable Baselines Jax) Implementation

[ä¿®æ­£ç‰ˆ]
- é€‚é…äº†é‡æ„åçš„ OLGEnvV9SAC (å…¨åŠŸèƒ½ç¯å¢ƒ) å’Œ HHSimulation_olgm_rl (å…¨åŠŸèƒ½æ¨¡æ‹Ÿå™¨)ã€‚
- è®­ç»ƒç¯å¢ƒç°åœ¨æ˜¯ OLGEnvV9SACï¼Œå®ƒåœ¨æ¯è½®å¼€å§‹æ—¶ä¼šé‡‡æ ·ä¸åŒçš„å®è§‚å˜é‡ Mã€‚
- è¯„ä¼°ç¯å¢ƒä¹Ÿæ˜¯ OLGEnvV9SACï¼Œä½†å®è§‚å˜é‡è¢«å›ºå®šï¼Œä»¥è¿›è¡Œç¨³å®šè¯„ä¼°ã€‚
- ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿè¯„ä¼°å‡½æ•° evaluate_... å·²æ›´æ–°ï¼Œä»¥è°ƒç”¨æ–°çš„ HHSimulation_olgm_rlï¼Œå¹¶æ­£ç¡®ä¼ é€’å‚æ•°ã€‚
- ç¡®ä¿äº†ä»è®­ç»ƒåˆ°è¯„ä¼°å†åˆ°æœ€ç»ˆæ¯”è¾ƒçš„é€»è¾‘ä¸€è‡´æ€§ã€‚
"""

import numpy as np
import jax
import pickle
import time
import os
from typing import Dict, Any, Tuple
from sbx import SAC
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold
from stable_baselines3.common.monitor import Monitor
import matplotlib.pyplot as plt

# [ä¿®æ­£] å¯¼å…¥æ–°çš„å…¨åŠŸèƒ½ç¯å¢ƒå’Œæ¨¡æ‹Ÿå™¨
from main_olg_v9_utils import OLG_V9_Utils, OLGEnvV9SAC, TempParamSHH


# --- [æ–°å¢] ä¸“é—¨ç”¨äºè¯„ä¼°æ¨¡å¼çš„ç»˜å›¾å‡½æ•° ---
def plot_evaluation_results(results: Dict, cS: Any):
    """å¯è§†åŒ–è¯„ä¼°æ¨¡å¼çš„ç»“æœ"""
    print("\nğŸ“ˆ ç”Ÿæˆè¯„ä¼°ç»“æœå›¾è¡¨...")
    
    fig, axes = plt.subplots(2, 2, figsize=(20, 16))
    fig.suptitle('é¢„è®­ç»ƒRLæ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°', fontsize=20, y=0.97)
    axes = axes.flatten()
    
    # å›¾A: ç»ˆèº«æ•ˆç”¨åˆ†å¸ƒ
    mean_u = results['mean_utility_rl']
    std_u = results['std_utility_rl']
    axes[0].hist(results['lifetime_utility_rl'], bins=30, density=True, alpha=0.7, label='æ•ˆç”¨åˆ†å¸ƒ')
    axes[0].axvline(mean_u, color='r', linestyle='--', label=f'å¹³å‡å€¼: {mean_u:.2f}')
    axes[0].set_title(f'A. ç»ˆèº«æ•ˆç”¨åˆ†å¸ƒ (Î¼={mean_u:.2f}, Ïƒ={std_u:.2f})', fontsize=16)
    axes[0].set_xlabel('ç»ˆèº«æœŸæœ›æ•ˆç”¨', fontsize=12)
    axes[0].set_ylabel('æ¦‚ç‡å¯†åº¦', fontsize=12)
    axes[0].legend()
    axes[0].grid(True, linestyle='--', alpha=0.6)

    age_groups = np.arange(cS.aD_new)
    
    # å›¾B: å¹³å‡æ¶ˆè´¹è·¯å¾„
    axes[1].plot(age_groups, np.mean(results['c_path_rl'], axis=0), 'o-', color='red', label='å¹³å‡æ¶ˆè´¹')
    axes[1].set_title('B. å¹³å‡æ¶ˆè´¹ç”Ÿå‘½å‘¨æœŸè·¯å¾„', fontsize=16)
    axes[1].set_xlabel('å¹´é¾„ç»„', fontsize=12)
    axes[1].set_ylabel('æ¶ˆè´¹ (c)', fontsize=12)
    axes[1].legend()
    axes[1].grid(True, linestyle='--', alpha=0.6)
    
    # å›¾C: å¹³å‡èµ„äº§è·¯å¾„
    axes[2].plot(age_groups, np.mean(results['k_path_rl'], axis=0), 's--', color='blue', label='å¹³å‡éPPSèµ„äº§ (k)')
    axes[2].set_title('C. å¹³å‡èµ„äº§ç”Ÿå‘½å‘¨æœŸè·¯å¾„', fontsize=16)
    axes[2].set_xlabel('å¹´é¾„ç»„', fontsize=12)
    axes[2].set_ylabel('èµ„äº§ (k)', fontsize=12)
    axes[2].legend()
    axes[2].grid(True, linestyle='--', alpha=0.6)

    # å›¾D: å¹³å‡PPSç¼´è´¹è·¯å¾„
    axes[3].plot(age_groups, np.mean(results['cpps_path_rl'], axis=0), '^:', color='green', label='å¹³å‡PPSç¼´è´¹')
    axes[3].axvline(x=cS.aR_new - 1, color='gray', linestyle='--', label='é€€ä¼‘å¹´é¾„')
    axes[3].set_title('D. å¹³å‡PPSç¼´è´¹ç”Ÿå‘½å‘¨æœŸè·¯å¾„', fontsize=16)
    axes[3].set_xlabel('å¹´é¾„ç»„', fontsize=12)
    axes[3].set_ylabel('PPSç¼´è´¹ (c_pps)', fontsize=12)
    axes[3].legend()
    axes[3].grid(True, linestyle='--', alpha=0.6)

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    save_path = './py/rl_agent_evaluation.png'
    plt.savefig(save_path, dpi=300)
    print(f"âœ… è¯„ä¼°å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
    plt.show()


# --- [æ–°å¢] ä¸“é—¨çš„è¯„ä¼°æ¨¡å¼ä¸»å‡½æ•° ---
def run_evaluation_only():
    """ä»…åŠ è½½å¹¶è¯„ä¼°æœ€ä½³æ¨¡å‹"""
    print("\n" + "="*80)
    print("ğŸš€ è¿›å…¥è¯„ä¼°æ¨¡å¼ (Evaluation-Only Mode)")
    print("="*80)

    # 1. å®šä¹‰æ¨¡å‹å’Œé…ç½®è·¯å¾„
    best_model_path = './py/best_model_sbx_full/best_model.zip'
    config_path = best_model_path.replace('.zip', '_config.pkl')

    if not os.path.exists(best_model_path) or not os.path.exists(config_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹ '{best_model_path}' æˆ–é…ç½®æ–‡ä»¶ '{config_path}'ã€‚")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒæ¨¡å¼ä»¥ç”Ÿæˆæ¨¡å‹ã€‚")
        return

    # 2. åŠ è½½æ¨¡å‹å’Œé…ç½®
    print(f"ğŸ“ æ­£åœ¨åŠ è½½æ¨¡å‹: {best_model_path}")
    model = SAC.load(best_model_path)
    print(f"ğŸ“ æ­£åœ¨åŠ è½½é…ç½®: {config_path}")
    with open(config_path, 'rb') as f:
        config = pickle.load(f)
    
    cS = config['cS']
    paramS_for_rl = config['paramS_for_rl']
    rng_M = config['rng_M']
    
    # 3. è¿è¡Œå¤§è§„æ¨¡æ¨¡æ‹Ÿè¯„ä¼°
    # ä½¿ç”¨ä¸æœ€ç»ˆæ¯”è¾ƒè„šæœ¬(test_vfi_grid_search.py)å®Œå…¨ä¸€è‡´çš„è¯„ä¼°ç¯å¢ƒ
    mean_reward, std_reward, lifecycle_results = evaluate_policy_lifecycle_simulation_age_group(
        model, cS, paramS_for_rl, rng_M, 
        n_sim=1000, # ä½¿ç”¨æ›´å¤šçš„æ¨¡æ‹Ÿä¸ªä½“ä»¥è·å¾—æ›´å¹³æ»‘çš„è·¯å¾„
        random_seed=42, 
        verbose=True
    )
    
    print("\n" + "="*50)
    print("ğŸ“ˆ æœ€ç»ˆè¯„ä¼°ç»“æœ")
    print(f"   å¹³å‡ç»ˆèº«æ•ˆç”¨: {mean_reward:.4f} Â± {std_reward:.4f}")
    print("="*50)

    # 4. å¯è§†åŒ–ç»“æœ
    plot_evaluation_results(lifecycle_results, cS)

def evaluate_policy_lifecycle_simulation_age_group(
    model: Any, cS: Any, paramS_for_rl: Dict, rng_M: Dict, 
    n_sim: int = 50, deterministic: bool = True, 
    random_seed: int = 42, verbose: bool = True,
    eIdxM_group_input: np.ndarray = None) -> Tuple[float, float, Dict]:
    """
    [ä¿®æ­£ç‰ˆ] ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿè¯„ä¼°RLæ¨¡å‹ï¼Œé€‚é…å…¨åŠŸèƒ½ç¯å¢ƒå’Œæ¨¡æ‹Ÿå™¨ã€‚
    
    æ­¤å‡½æ•°ç°åœ¨è°ƒç”¨æ–°çš„ HHSimulation_olgm_rlï¼Œè¯¥å‡½æ•°å†…éƒ¨å¤„ç†ä¸ OLGEnvV9SAC
    ç¯å¢ƒçš„äº¤äº’ï¼Œç¡®ä¿é€»è¾‘ä¸€è‡´ã€‚
    """
    sim_start_time = time.time()

    # 1. è®¾ç½®å›ºå®šçš„å®è§‚ç»æµå‚æ•°ç”¨äºè¯„ä¼°
    M_fixed = {
        'R_k_net_factor': 1.03, 'w_gross': 2.0, 'TR_total': 0.1,
        'b_payg_avg_retiree': 0.4, 'tau_l': 0.15, 'theta_payg_actual': 0.12
    }
    
    if verbose:
        print(f"\nğŸ§¬ å¹´é¾„ç»„ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿè¯„ä¼° (å…¨åŠŸèƒ½ç¯å¢ƒç‰ˆ)")
        print(f"   (n_sim={n_sim}, seed={random_seed})")
        print("ğŸ“Š å›ºå®šæµ‹è¯•å‚æ•°:", M_fixed)

    # 2. ç”Ÿæˆæˆ–ä½¿ç”¨é¢„è®¾çš„æ•ˆç‡å†²å‡»è·¯å¾„
    np.random.seed(random_seed)
    aD_new = int(cS.aD_new)
    if eIdxM_group_input is None:
        if verbose: print("ğŸ”„ ç”Ÿæˆå¹´é¾„ç»„æ•ˆç‡å†²å‡»è·¯å¾„...")
        eIdxM_group = OLG_V9_Utils.MarkovChainSimulation_AgeGroup(
            n_sim, cS, paramS_for_rl['leProb1V'], paramS_for_rl['leTrProbM'])
    else:
        if verbose: print("ğŸ”„ ä½¿ç”¨é¢„ç”Ÿæˆçš„å¹´é¾„ç»„æ•ˆç‡å†²å‡»è·¯å¾„...")
        eIdxM_group = eIdxM_group_input
    
    # [ä¿®æ­£] eIdxM_group éœ€è¦æ˜¯1-basedç´¢å¼•
    eIdxM_group_1based = eIdxM_group + 1

    # 3. å‡†å¤‡æ¨¡æ‹Ÿå‚æ•°
    bV_payg_group = np.zeros(aD_new)
    if int(cS.aR_new) < aD_new:
        bV_payg_group[int(cS.aR_new):] = M_fixed['b_payg_avg_retiree']

    # [æ–°] æ¨¡æ‹Ÿå™¨éœ€è¦ä¸€ä¸ªåŒ…å«æ‰€æœ‰è®­ç»ƒæ—¶å‚æ•°çš„é…ç½®å­—å…¸
    rl_config = {'cS': cS, 'paramS_for_rl': paramS_for_rl, 'rng_M': rng_M}
    
    # [æ–°] æ¨¡æ‹Ÿå™¨è¿˜éœ€è¦ä¸€ä¸ªä¸´æ—¶çš„ paramS å¯¹è±¡ï¼ŒåŒ…å«å½“å‰è¿­ä»£çš„ç¨ç‡ç­‰
    paramS_sim = TempParamSHH(
        M_fixed['tau_l'],
        M_fixed['theta_payg_actual'],
        cS.pps_active,
        cS.ageEffV_new
    )
    
    # 4. [æ ¸å¿ƒä¿®æ­£] è°ƒç”¨ä¸å…¨åŠŸèƒ½ç¯å¢ƒé€‚é…çš„ HHSimulation_olgm_rl æ¨¡æ‹Ÿå™¨
    if verbose: print("ğŸš€ è°ƒç”¨ HHSimulation_olgm_rl è¿›è¡Œæ ¸å¿ƒæ¨¡æ‹Ÿ...")
    k_path_rl, kpps_path_rl, c_path_rl, cpps_path_rl = OLG_V9_Utils.HHSimulation_olgm_rl(
        model, rl_config, eIdxM_group_1based,
        M_fixed['R_k_net_factor'], M_fixed['w_gross'], M_fixed['TR_total'],
        bV_payg_group, paramS_sim, cS
    )
    if verbose: print("âœ… æ ¸å¿ƒæ¨¡æ‹Ÿå®Œæˆã€‚")

    # 5. è®¡ç®—ç”Ÿå‘½å‘¨æœŸæ•ˆç”¨ (æ­¤éƒ¨åˆ†é€»è¾‘ä¸å˜)
    if verbose: print("ğŸ“Š è®¡ç®—VFIç­‰ä»·çš„ç”Ÿå‘½å‘¨æœŸæ•ˆç”¨...")
    lifetime_utility_rl = np.zeros(n_sim)
    beta = float(cS.beta)
    s_transitionV = cS.s_1yr_transitionV.flatten()
    for i in range(n_sim):
        utility_sum = 0.0
        cumulative_discount = 1.0
        for a_group in range(aD_new):
            c = c_path_rl[i, a_group]
            _, u = OLG_V9_Utils.CES_utility(c, cS.sigma, cS)
            utility_sum += cumulative_discount * u
            if a_group < aD_new - 1:
                cumulative_discount *= (beta * s_transitionV[a_group])
        lifetime_utility_rl[i] = utility_sum

    mean_utility_rl = np.mean(lifetime_utility_rl)
    std_utility_rl = np.std(lifetime_utility_rl)
    sim_time = time.time() - sim_start_time

    if verbose:
        print(f"âœ… è¯„ä¼°å®Œæˆï¼Œè€—æ—¶: {sim_time:.2f} ç§’")
        print(f"ğŸ“Š RLç”Ÿå‘½å‘¨æœŸæ•ˆç”¨: {mean_utility_rl:.4f} Â± {std_utility_rl:.4f}")
    
    # 6. æ•´ç†å¹¶è¿”å›ç»“æœ
    lifecycle_results = {
        'mean_utility_rl': mean_utility_rl,
        'std_utility_rl': std_utility_rl,
        'lifetime_utility_rl': lifetime_utility_rl,
        'k_path_rl': k_path_rl,
        'c_path_rl': c_path_rl,
        'cpps_path_rl': cpps_path_rl,
        'eIdxM_group': eIdxM_group, # è¿”å›0-basedç´¢å¼•
    }
    return mean_utility_rl, std_utility_rl, lifecycle_results


class EvalCallbackWithDiscount(EvalCallback):
    """[ä¿®æ­£ç‰ˆ] è‡ªå®šä¹‰EvalCallbackï¼Œé€‚é…å…¨åŠŸèƒ½ç¯å¢ƒè¯„ä¼°"""
    
    def __init__(self, eval_env, cS, paramS_for_rl, rng_M, **kwargs):
        super().__init__(eval_env, **kwargs)
        self.cS = cS
        self.paramS_for_rl = paramS_for_rl
        self.rng_M = rng_M
        self._generate_unified_efficiency_shocks()
        print(f"ğŸ”§ ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸæ¨¡æ‹Ÿè¯„ä¼°å›è°ƒ (VFIç­‰ä»·)")

    def _generate_unified_efficiency_shocks(self):
        print("ğŸ² ç”Ÿæˆç»Ÿä¸€æ•ˆç‡å†²å‡»åºåˆ—ï¼ˆç”¨äºæ‰€æœ‰è¯„ä¼°ï¼‰...")
        np.random.seed(42) # å›ºå®šç§å­
        n_sim_target = max(self.n_eval_episodes, 100)
        self.eIdxM_group_unified = OLG_V9_Utils.MarkovChainSimulation_AgeGroup(
            n_sim_target, self.cS, self.paramS_for_rl['leProb1V'], self.paramS_for_rl['leTrProbM']
        )
        print(f"âœ… ç»Ÿä¸€æ•ˆç‡å†²å‡»çŸ©é˜µç”Ÿæˆå®Œæˆ: {self.eIdxM_group_unified.shape}")

    def _on_step(self) -> bool:
        # [ç®€åŒ–] åªä¿ç•™æ ¸å¿ƒçš„è¯„ä¼°å’Œä¿å­˜é€»è¾‘
        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:
            
            # [æ ¸å¿ƒä¿®æ­£] è°ƒç”¨é€‚é…æ–°ç¯å¢ƒçš„è¯„ä¼°å‡½æ•°
            mean_reward, std_reward, _ = evaluate_policy_lifecycle_simulation_age_group(
                self.model, self.cS, self.paramS_for_rl, self.rng_M,
                n_sim=self.n_eval_episodes, 
                deterministic=self.deterministic,
                random_seed=42,
                verbose=False,
                eIdxM_group_input=self.eIdxM_group_unified
            )
            
            if self.verbose > 0:
                print(f"Eval @ T={self.num_timesteps} - Mean reward: {mean_reward:.4f} +/- {std_reward:.4f}")
            
            self.logger.record("eval/mean_reward", mean_reward)
            self.logger.dump(self.num_timesteps)

            if mean_reward > self.best_mean_reward:
                self.best_mean_reward = mean_reward
                if self.verbose > 0:
                    print(f"ğŸ† New best mean reward: {self.best_mean_reward:.4f}")
                if self.best_model_save_path is not None:
                    self.model.save(os.path.join(self.best_model_save_path, "best_model"))
            
            # (å¯é€‰) å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ åœæ­¢è®­ç»ƒçš„å›è°ƒé€»è¾‘
            if self.callback is not None:
                 return self.callback.on_step()
                 
        return True


def main(args):
    """ä¸»å‡½æ•°ï¼Œæ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ¨¡å¼"""
    
    # [æ–°å¢] æ ¹æ® --eval_only æ ‡å¿—é€‰æ‹©æ‰§è¡Œè·¯å¾„
    if args.eval_only:
        run_evaluation_only()
        return # è¯„ä¼°åç›´æ¥é€€å‡º

    # --- ä»¥ä¸‹æ˜¯åŸå§‹çš„è®­ç»ƒæµç¨‹ ---
    print("=== OLG æ¨¡å‹ V9 - SAC æ™ºèƒ½ä½“è®­ç»ƒ (å…¨åŠŸèƒ½ç¯å¢ƒç‰ˆ) ===")
    
    # 1. åˆå§‹åŒ–å‚æ•°
    print("\n--- 1. åˆå§‹åŒ–å‚æ•° ---")
    cS = OLG_V9_Utils.ParameterValues_HuggettStyle()
    leLogGridV, leTrProbM, leProb1V = OLG_V9_Utils.EarningProcess_olgm(cS)
    paramS_for_rl = {
        'leLogGridV': leLogGridV, 'leTrProbM': leTrProbM, 'leProb1V': leProb1V,
        'leGridV': np.exp(leLogGridV), 'ageEffV_new': cS.ageEffV_new
    }
    
    # 2. å®šä¹‰å®è§‚çŠ¶æ€Mçš„é‡‡æ ·èŒƒå›´
    rng_M = {
        'R_k_net_factor': [1.01, 1.05], 'w_gross': [1.5, 2.5],
        'TR_total': [0.0, 0.2], 'b_payg_avg_retiree': [0.1, 0.8],
        'tau_l': [0.05, 0.25], 'theta_payg_actual': [0.05, 0.20]
    }
    
    # 3. åˆ›å»ºè®­ç»ƒç¯å¢ƒ
    print("\n--- 3. åˆ›å»ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒ ---")
    env = OLGEnvV9SAC(cS, paramS_for_rl, rng_M, training_mode=True)
    env = Monitor(env)
    
    # 4. åˆ›å»ºSAC Agent
    model_kwargs = {
        'policy': 'MlpPolicy', 
        'env': env, 
        'learning_rate': 1e-4,
        'buffer_size': int(1e6), 
        'batch_size': 512, 
        'tau': 5e-3,
        'gamma': 0.97, 
        'ent_coef': 'auto',
        'policy_kwargs': {'net_arch': [384, 384]},
        'verbose': 1, 
        'learning_starts': 5000, 
        'seed': 42,
        'tensorboard_log': './py/tensorboard_logs_sbx/'
    }
    model = SAC(**model_kwargs)
    
    # 5. è®¾ç½®è¯„ä¼°å›è°ƒ
    print("\n--- 5. è®¾ç½®è¯„ä¼°å›è°ƒ ---")
    # [ä¿®æ­£] ç¡®ä¿è¯„ä¼°ç¯å¢ƒçš„å®è§‚å˜é‡èŒƒå›´ä¸è®­ç»ƒæ—¶ä¸€è‡´
    eval_env = OLGEnvV9SAC(cS, paramS_for_rl, rng_M, training_mode=False)
    eval_env = Monitor(eval_env)
    eval_callback = EvalCallbackWithDiscount(
        eval_env, cS, paramS_for_rl, rng_M,
        best_model_save_path='./py/best_model_sbx_full/',
        log_path='./py/logs_sbx_full/',
        eval_freq=5000, n_eval_episodes=100, deterministic=True, verbose=1
    )
    
    # 6. å¼€å§‹è®­ç»ƒ
    print("\n--- 6. å¼€å§‹è®­ç»ƒ ---")
    os.makedirs('./py/best_model_sbx_full/', exist_ok=True)
    os.makedirs('./py/logs_sbx_full/', exist_ok=True)
    
    start_time = time.time()
    model.learn(total_timesteps=500_000, callback=eval_callback, progress_bar=True)
    training_time = time.time() - start_time
    print(f"è®­ç»ƒç”¨æ—¶: {training_time:.2f} ç§’")
    
    # 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œé…ç½®
    print("\n--- 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œé…ç½® ---")
    final_model_path = './py/final_sac_agent_olg_sbx_full.zip'
    model.save(final_model_path.replace('.zip', ''))
    config = {
        'cS': cS, 'paramS_for_rl': paramS_for_rl, 'rng_M': rng_M,
        'model_kwargs': {k: v for k, v in model_kwargs.items() if k != 'env'},
        'training_time': training_time, 'algorithm': 'SBX_SAC_Full_Env'
    }
    with open(final_model_path.replace('.zip', '_config.pkl'), 'wb') as f:
        pickle.dump(config, f)
    best_model_path = './py/best_model_sbx_full/best_model.zip'
    if os.path.exists(best_model_path):
        with open(best_model_path.replace('.zip', '_config.pkl'), 'wb') as f:
            pickle.dump(config, f)
            
    print(f"æœ€ç»ˆæ¨¡å‹ä¿å­˜åœ¨: {final_model_path}")
    print("è®­ç»ƒå’Œé…ç½®ä¿å­˜å®Œæˆã€‚")
    
    # 8. æœ€ç»ˆè¯„ä¼°
    print("\n--- 8. æœ€ç»ˆè¯„ä¼° ---")
    mean_reward, std_reward, _ = evaluate_policy_lifecycle_simulation_age_group(
        model, cS, paramS_for_rl, rng_M, n_sim=200, random_seed=42, verbose=True
    )
    print(f"\næœ€ç»ˆæ¨¡å‹åœ¨å›ºå®šå®è§‚ç¯å¢ƒä¸‹çš„ç”Ÿå‘½å‘¨æœŸæ•ˆç”¨: {mean_reward:.4f} +/- {std_reward:.4f}")


import argparse # [æ–°å¢] å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°æ¨¡å—

if __name__ == "__main__":
    # [æ–°å¢] è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="è®­ç»ƒæˆ–è¯„ä¼°OLGæ¨¡å‹çš„SACæ™ºèƒ½ä½“ã€‚")
    parser.add_argument(
        '--eval_only',
        action='store_true',
        help='å¦‚æœè®¾ç½®æ­¤æ ‡å¿—ï¼Œåˆ™åªè¿è¡Œè¯„ä¼°æ¨¡å¼ï¼Œä¸è¿›è¡Œè®­ç»ƒã€‚'
    )
    args = parser.parse_args()
    
    main(args)
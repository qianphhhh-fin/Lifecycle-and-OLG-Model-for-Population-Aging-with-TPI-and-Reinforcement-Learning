# --- START OF FILE sac_vfi_equivalent.py ---

# =======================================================================================
# === VFI-Equivalent SAC Algorithm for OLG Model (SBX JAX Implementation)           ===
# =======================================================================================
#
# è¿™ä¸ªæ–‡ä»¶å®ç°äº†å¯¹SBX SACç®—æ³•çš„ä¿®æ”¹ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†å¸¦æœ‰çŠ¶æ€ä¾èµ–è´´ç°å› å­ï¼ˆå­˜æ´»ç‡ï¼‰
# çš„åŠ¨æ€è§„åˆ’é—®é¢˜ã€‚è¿™æ˜¯é€šè¿‡é‡å†™SACç±»çš„æ ¸å¿ƒè®­ç»ƒé€»è¾‘æ¥å®ç°çš„ï¼Œç¡®ä¿å…¶Bellmanæ–¹ç¨‹
# ä¸OLGæ¨¡å‹çš„VFIç†è®ºå®Œå…¨ç­‰ä»·ã€‚
#
# æ ¸å¿ƒä¿®æ”¹:
# - åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„ç±» `SACWithSurvivalDiscount`ï¼Œç»§æ‰¿è‡ª `sbx.SAC`ã€‚
# - é‡å†™äº† `update_critic` æ–¹æ³•ï¼Œè¿™æ˜¯è®¡ç®—ç›®æ ‡Qå€¼çš„å…³é”®æ‰€åœ¨ã€‚
# - åœ¨è®¡ç®—ç›®æ ‡Qå€¼æ—¶ï¼Œå°†æ ‡å‡†çš„ `gamma` è´´ç°æ›¿æ¢ä¸º `beta * survival_prob`ï¼Œ
#   å…¶ä¸­ `beta` å’Œ `survival_prob` ä»ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­è·å–ã€‚
# - è¿™ä½¿å¾—RLæ™ºèƒ½ä½“ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°ä¸VFIçš„Bellmanæ–¹ç¨‹ `V=u+Î²*s*E[V']` åœ¨æ•°å­¦ä¸Š
#   å®Œå…¨ç­‰ä»·ï¼Œä»è€Œå®ç°äº†ç†è®ºä¸€è‡´æ€§ã€‚
#
# ä½¿ç”¨æ–¹æ³•:
# - åœ¨è®­ç»ƒè„šæœ¬ä¸­ï¼Œç”¨ `SACWithSurvivalDiscount` æ›¿ä»£ `SAC`ã€‚
# - ç¡®ä¿ç¯å¢ƒçš„ `step` å‡½æ•°è¿”å›çš„ `info` å­—å…¸ä¸­åŒ…å« `survival_prob` å’Œ `beta`ã€‚
# - è®­ç»ƒç¯å¢ƒåº”è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆ`training_mode=False`ï¼‰ï¼Œä½¿å…¶å¥–åŠ±ä¸ºçº¯æ•ˆç”¨ `u(c)`ã€‚
# =======================================================================================

from functools import partial
from typing import Any, Dict, Tuple

import jax
import jax.numpy as jnp
import numpy as np
import optax
from flax.training.train_state import TrainState

from sbx.sac.sac import SAC, RLTrainState


class SACWithSurvivalDiscount(SAC):
    """
    ä¸€ä¸ªè‡ªå®šä¹‰çš„SACç±»ï¼Œå®ƒä¿®æ”¹äº†Criticçš„æ›´æ–°è§„åˆ™ï¼Œä»¥å¤„ç†çŠ¶æ€ä¾èµ–çš„è´´ç°å› å­ã€‚
    è¿™ä½¿å¾—ç®—æ³•èƒ½å¤Ÿç²¾ç¡®åœ°æ±‚è§£å…·æœ‰å­˜æ´»æ¦‚ç‡çš„OLGæ¨¡å‹ï¼Œå®ç°äº†ä¸VFIç†è®ºçš„ç­‰ä»·æ€§ã€‚
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if self.verbose > 0:
            print("="*60)
            print("ğŸ”§ Initializing SACWithSurvivalDiscount")
            print("   - Critic update rule will be modified to use VFI-equivalent discounting.")
            print("   - Target Q-value formula: u(c) + beta * survival_prob * E[V(s')]")
            print("   - This requires the environment to provide 'survival_prob' and 'beta' in the info dict.")
            print("="*60)

    @staticmethod
    @jax.jit
    def update_critic(
        gamma: float,  # Note: The standard gamma is passed but WILL BE IGNORED.
        actor_state: TrainState,
        qf_state: RLTrainState,
        ent_coef_state: TrainState,
        observations: jax.Array,
        actions: jax.Array,
        next_observations: jax.Array,
        rewards: jax.Array,
        dones: jax.Array,
        # ------------------- æ ¸å¿ƒä¿®æ”¹ï¼šæ¥æ”¶é¢å¤–ä¿¡æ¯ -------------------
        betas: jax.Array,
        survival_probs: jax.Array,
        # -----------------------------------------------------------
        key: jax.Array,
    ):
        """
        é‡å†™çš„Criticæ›´æ–°å‡½æ•°ã€‚
        """
        key, noise_key, dropout_key_target, dropout_key_current = jax.random.split(key, 4)
        
        # 1. è®¡ç®—ä¸‹ä¸€çŠ¶æ€çš„ä»·å€¼ V(s')
        # é‡‡æ ·ä¸‹ä¸€çŠ¶æ€çš„åŠ¨ä½œ
        dist = actor_state.apply_fn(actor_state.params, next_observations)
        next_state_actions = dist.sample(seed=noise_key)
        next_log_prob = dist.log_prob(next_state_actions)

        # è·å–ç†µç³»æ•°
        ent_coef_value = ent_coef_state.apply_fn({"params": ent_coef_state.params})

        # è®¡ç®—ä¸‹ä¸€çŠ¶æ€çš„ç›®æ ‡Qå€¼ Q(s', a')
        qf_next_values = qf_state.apply_fn(
            qf_state.target_params,
            next_observations,
            next_state_actions,
            rngs={"dropout": dropout_key_target},
        )
        
        # ä½¿ç”¨ä¸¤ä¸ªCriticä¸­è¾ƒå°çš„ä¸€ä¸ª
        next_q_values = jnp.min(qf_next_values, axis=0)
        
        # SACçš„ç†µæ­£åˆ™åŒ–é¡¹: V(s') = Q(s', a') - alpha * log(pi(a'|s'))
        next_q_values = next_q_values - ent_coef_value * next_log_prob.reshape(-1, 1)

        # 2. ------------------- æ ¸å¿ƒä¿®æ”¹ç‚¹: è®¡ç®—Bellmanç›®æ ‡ -------------------
        # åŸå§‹çš„ target_q è®¡ç®—æ–¹å¼:
        # target_q_values = rewards.reshape(-1, 1) + (1 - dones.reshape(-1, 1)) * gamma * next_q_values
        
        # ä¿®æ­£åçš„ target_q è®¡ç®—æ–¹å¼ (VFI-equivalent):
        # reward å°±æ˜¯çº¯æ•ˆç”¨ u(c)
        # æˆ‘ä»¬ç”¨ beta * survival_prob * V(s') ä½œä¸ºæœªæ¥ä»·å€¼çš„è´´ç°
        target_q_values = rewards.reshape(-1, 1) + (1 - dones.reshape(-1, 1)) * betas.reshape(-1, 1) * survival_probs.reshape(-1, 1) * next_q_values
        # ---------------------------------------------------------------------

        # 3. è®¡ç®—CriticæŸå¤± (MSE)
        def mse_loss(params: Any, dropout_key: jax.Array) -> jax.Array:
            # current_q_values çš„å½¢çŠ¶æ˜¯ (n_critics, batch_size, 1)
            current_q_values = qf_state.apply_fn(params, observations, actions, rngs={"dropout": dropout_key})
            # æŸå¤±æ˜¯ä¸¤ä¸ªCriticçš„MSEä¹‹å’Œ
            return 0.5 * ((target_q_values - current_q_values) ** 2).mean(axis=1).sum()

        qf_loss_value, grads = jax.value_and_grad(mse_loss, has_aux=False)(qf_state.params, dropout_key_current)
        qf_state = qf_state.apply_gradients(grads=grads)

        return (
            qf_state,
            (qf_loss_value, ent_coef_value),
            key,
        )

    # æˆ‘ä»¬éœ€è¦é‡å†™ _train æ–¹æ³•ï¼Œå› ä¸ºå®ƒè°ƒç”¨äº† update_criticã€‚
    # æˆ‘ä»¬å°†ä»çˆ¶ç±»å¤åˆ¶_trainæ–¹æ³•ï¼Œå¹¶åªä¿®æ”¹å¯¹ update_critic çš„è°ƒç”¨ã€‚
    @classmethod
    @partial(jax.jit, static_argnames=["cls", "gradient_steps", "policy_delay", "policy_delay_offset"])
    def _train(
        cls,
        gamma: float,
        tau: float,
        target_entropy: jax.Array,
        gradient_steps: int,
        data: Dict, # ä¿®æ”¹ä¸ºå­—å…¸ä»¥åŒ…å«é¢å¤–ä¿¡æ¯
        policy_delay: int,
        policy_delay_offset: int,
        qf_state: RLTrainState,
        actor_state: TrainState,
        ent_coef_state: TrainState,
        key: jax.Array,
    ):
        """
        é‡å†™çš„æ ¸å¿ƒè®­ç»ƒå¾ªç¯ã€‚
        """
        # æ³¨æ„: dataç°åœ¨æ˜¯ä¸€ä¸ªåŒ…å« 'observations', 'actions', 'infos', etc. çš„å­—å…¸
        batch_size = data["observations"].shape[0] // gradient_steps

        carry = {
            "actor_state": actor_state,
            "qf_state": qf_state,
            "ent_coef_state": ent_coef_state,
            "key": key,
            "info": {
                "actor_loss": jnp.array(0.0),
                "qf_loss": jnp.array(0.0),
                "ent_coef_loss": jnp.array(0.0),
                "ent_coef_value": jnp.array(0.0),
            },
        }

        def one_update(i: int, carry: dict[str, Any]) -> dict[str, Any]:
            actor_state = carry["actor_state"]
            qf_state = carry["qf_state"]
            ent_coef_state = carry["ent_coef_state"]
            key = carry["key"]
            info = carry["info"]
            
            # ä»æ•°æ®å­—å…¸ä¸­åˆ‡ç‰‡
            batch_obs = jax.lax.dynamic_slice_in_dim(data["observations"], i * batch_size, batch_size)
            batch_act = jax.lax.dynamic_slice_in_dim(data["actions"], i * batch_size, batch_size)
            batch_next_obs = jax.lax.dynamic_slice_in_dim(data["next_observations"], i * batch_size, batch_size)
            batch_rew = jax.lax.dynamic_slice_in_dim(data["rewards"], i * batch_size, batch_size)
            batch_done = jax.lax.dynamic_slice_in_dim(data["dones"], i * batch_size, batch_size)
            # ------------------- æ ¸å¿ƒä¿®æ”¹ï¼šåˆ‡ç‰‡é¢å¤–ä¿¡æ¯ -------------------
            batch_betas = jax.lax.dynamic_slice_in_dim(data["infos"]["beta"], i * batch_size, batch_size)
            batch_survival_probs = jax.lax.dynamic_slice_in_dim(data["infos"]["survival_prob"], i * batch_size, batch_size)
            # -------------------------------------------------------------

            # è°ƒç”¨æˆ‘ä»¬é‡å†™çš„ update_critic æ–¹æ³•
            (
                qf_state,
                (qf_loss_value, ent_coef_value),
                key,
            ) = cls.update_critic(
                gamma, # gamma is ignored inside
                actor_state,
                qf_state,
                ent_coef_state,
                batch_obs,
                batch_act,
                batch_next_obs,
                batch_rew,
                batch_done,
                # ------------------- æ ¸å¿ƒä¿®æ”¹ï¼šä¼ é€’é¢å¤–ä¿¡æ¯ -------------------
                batch_betas,
                batch_survival_probs,
                # -----------------------------------------------------------
                key,
            )
            qf_state = cls.soft_update(tau, qf_state)

            # Actorå’Œç†µçš„æ›´æ–°ä¿æŒä¸å˜
            (actor_state, qf_state, ent_coef_state, actor_loss_value, ent_coef_loss_value, key) = jax.lax.cond(
                (policy_delay_offset + i) % policy_delay == 0,
                cls.update_actor_and_temperature,
                lambda *_: (actor_state, qf_state, ent_coef_state, info["actor_loss"], info["ent_coef_loss"], key),
                actor_state,
                qf_state,
                ent_coef_state,
                batch_obs,
                target_entropy,
                key,
            )
            info = {
                "actor_loss": actor_loss_value,
                "qf_loss": qf_loss_value,
                "ent_coef_loss": ent_coef_loss_value,
                "ent_coef_value": ent_coef_value,
            }

            return {
                "actor_state": actor_state,
                "qf_state": qf_state,
                "ent_coef_state": ent_coef_state,
                "key": key,
                "info": info,
            }

        update_carry = jax.lax.fori_loop(0, gradient_steps, one_update, carry)

        return (
            update_carry["qf_state"],
            update_carry["actor_state"],
            update_carry["ent_coef_state"],
            update_carry["key"],
            (
                update_carry["info"]["actor_loss"],
                update_carry["info"]["qf_loss"],
                update_carry["info"]["ent_coef_loss"],
                update_carry["info"]["ent_coef_value"],
            ),
        )


    def train(self, gradient_steps: int, batch_size: int) -> None:
        """
        é‡å†™ train æ–¹æ³•ã€‚
        æ ¸å¿ƒä¿®æ”¹ï¼šåœ¨å°†æ•°æ®ä¼ é€’ç»™JITç¼–è¯‘çš„_trainæ–¹æ³•ä¹‹å‰ï¼Œ
        å°†æ‰€æœ‰PyTorch Tensorsè½¬æ¢ä¸ºNumPy arraysã€‚
        """
        if self.replay_buffer is None:
            raise ValueError("Replay buffer is not initialized")
        
        # 1. ä» CustomReplayBuffer ä¸­é‡‡æ ·ï¼Œè¿”å›åŒ…å« PyTorch Tensors çš„å¯¹è±¡
        samples: CustomReplayBufferSamples = self.replay_buffer.sample(
            batch_size * gradient_steps, env=self._vec_normalize_env
        )
        
        # 2. ğŸ”§ æ·»åŠ æ•°æ®æ ¼å¼éªŒè¯å’Œä¿®å¤
        survival_prob_tensor = samples.infos["survival_prob"]
        beta_tensor = samples.infos["beta"]
        
        # ç¡®ä¿æ•°æ®ç»´åº¦æ­£ç¡®
        if survival_prob_tensor.dim() == 0:
            survival_prob_tensor = survival_prob_tensor.unsqueeze(0)
        if beta_tensor.dim() == 0:
            beta_tensor = beta_tensor.unsqueeze(0)
            
        # ç¡®ä¿æ•°æ®é•¿åº¦åŒ¹é…
        expected_length = batch_size * gradient_steps
        if len(survival_prob_tensor) != expected_length:
            if self.verbose > 0:
                print(f"âš ï¸ Warning: survival_prob length mismatch. Expected {expected_length}, got {len(survival_prob_tensor)}")
        if len(beta_tensor) != expected_length:
            if self.verbose > 0:
                print(f"âš ï¸ Warning: beta length mismatch. Expected {expected_length}, got {len(beta_tensor)}")
        
        # 3. å‡†å¤‡ JAX è®­ç»ƒæ‰€éœ€çš„æ•°æ®å­—å…¸ï¼Œå¹¶è¿›è¡Œæ ¼å¼è½¬æ¢
        # ------------------- æ ¸å¿ƒä¿®æ”¹ç‚¹ -------------------
        # å°†æ‰€æœ‰ torch.Tensor è½¬æ¢ä¸º numpy.ndarray
        # JAX å¯ä»¥æ— ç¼å¤„ç† NumPy æ•°ç»„
        train_data = {
            "observations": samples.observations.cpu().numpy(),
            "actions": samples.actions.cpu().numpy(),
            "next_observations": samples.next_observations.cpu().numpy(),
            "dones": samples.dones.cpu().numpy().flatten(),
            "rewards": samples.rewards.cpu().numpy().flatten(),
            # infos å­—å…¸ä¸­çš„æ¯ä¸ªå€¼ä¹Ÿéœ€è¦è½¬æ¢
            "infos": {
                "survival_prob": survival_prob_tensor.cpu().numpy(),
                "beta": beta_tensor.cpu().numpy(),
            },
        }
        # ----------------------------------------------------
        
        # 4. ğŸ”§ æ·»åŠ æœ€ç»ˆéªŒè¯
        if self.verbose > 1:
            print(f"ğŸ“Š Training data shapes:")
            print(f"   observations: {train_data['observations'].shape}")
            print(f"   survival_prob: {train_data['infos']['survival_prob'].shape}")
            print(f"   beta: {train_data['infos']['beta'].shape}")
        
        # 5. æ›´æ–°å­¦ä¹ ç‡
        current_lr = self.lr_schedule(self._current_progress_remaining)
        self._update_learning_rate(self.policy.actor_state.opt_state, learning_rate=current_lr)
        
        qf_lr = self.qf_learning_rate if self.qf_learning_rate is not None else current_lr
        self._update_learning_rate(self.policy.qf_state.opt_state, learning_rate=qf_lr)

        # 6. è°ƒç”¨æ ¸å¿ƒçš„ _train å‡½æ•°æ‰§è¡Œæ¢¯åº¦æ›´æ–°
        # ç°åœ¨ä¼ é€’ç»™ _train çš„æ˜¯åŒ…å« NumPy æ•°ç»„çš„å­—å…¸ï¼ŒJAX å¯ä»¥å¤„ç†
        (
            self.policy.qf_state,
            self.policy.actor_state,
            self.ent_coef_state,
            self.key,
            (actor_loss_value, qf_loss_value, ent_coef_loss_value, ent_coef_value),
        ) = self._train(
            gamma=self.gamma,
            tau=self.tau,
            target_entropy=self.target_entropy,
            gradient_steps=gradient_steps,
            data=train_data,
            policy_delay=self.policy_delay,
            policy_delay_offset=(self._n_updates + 1) % self.policy_delay,
            qf_state=self.policy.qf_state,
            actor_state=self.policy.actor_state,
            ent_coef_state=self.ent_coef_state,
            key=self.key,
        )

        # 7. æ›´æ–°è®­ç»ƒè¿­ä»£æ¬¡æ•°å¹¶è®°å½•æ—¥å¿—
        self._n_updates += gradient_steps
        self.logger.record("train/n_updates", self._n_updates, exclude="tensorboard")
        self.logger.record("train/actor_loss", actor_loss_value.item())
        self.logger.record("train/critic_loss", qf_loss_value.item())
        self.logger.record("train/ent_coef_loss", ent_coef_loss_value.item())
        self.logger.record("train/ent_coef", ent_coef_value.item())

# --- START OF FILE custom_replay_buffer.py ---

# --- START OF FILE custom_replay_buffer.py (Version 2) ---

import warnings
from typing import Any, Dict, List, Optional, Union, NamedTuple
from dataclasses import dataclass

import numpy as np
import torch as th
from gymnasium import spaces

from stable_baselines3.common.buffers import ReplayBuffer
from stable_baselines3.common.type_aliases import ReplayBufferSamples
from stable_baselines3.common.vec_env import VecNormalize

# ------------------- æ ¸å¿ƒä¿®æ”¹ï¼šå®šä¹‰æˆ‘ä»¬è‡ªå·±çš„æ ·æœ¬æ•°æ®ç»“æ„ -------------------
@dataclass
class CustomReplayBufferSamples:
    """
    ä¸€ä¸ªè‡ªå®šä¹‰çš„æ•°æ®ç±»ï¼Œç”¨äºä»ReplayBufferä¸­ä¼ é€’æ ·æœ¬ã€‚
    å®ƒåŒ…å«äº†æ ‡å‡†æ ·æœ¬çš„æ‰€æœ‰å­—æ®µï¼Œå¹¶é¢å¤–æ·»åŠ äº†infoså­—å…¸ã€‚
    """
    observations: th.Tensor
    actions: th.Tensor
    next_observations: th.Tensor
    dones: th.Tensor
    rewards: th.Tensor
    infos: Dict[str, th.Tensor]
# -------------------------------------------------------------------------


class ReplayBufferOLG(ReplayBuffer):
    """
    ä¸€ä¸ªè‡ªå®šä¹‰çš„ReplayBufferï¼Œå®ƒç»§æ‰¿è‡ªæ ‡å‡†çš„ReplayBufferï¼Œä½†å¢åŠ äº†
    å¯¹ç‰¹å®šinfoå­—æ®µï¼ˆ'survival_prob'å’Œ'beta'ï¼‰çš„å­˜å‚¨å’Œé‡‡æ ·åŠŸèƒ½ã€‚
    """

    def __init__(
        self,
        buffer_size: int,
        observation_space: spaces.Space,
        action_space: spaces.Space,
        device: Union[th.device, str] = "auto",
        n_envs: int = 1,
        optimize_memory_usage: bool = False,
        handle_timeout_termination: bool = True,
    ):
        super().__init__(
            buffer_size,
            observation_space,
            action_space,
            device,
            n_envs=n_envs,
            optimize_memory_usage=optimize_memory_usage,
            handle_timeout_termination=handle_timeout_termination,
        )

        self.survival_probs = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)
        self.betas = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)

        print("ğŸ”§ Initializing CustomReplayBuffer (v2):")
        print("   - Using a custom dataclass for samples to include infos.")

    def add(
        self,
        obs: np.ndarray,
        next_obs: np.ndarray,
        action: np.ndarray,
        reward: np.ndarray,
        done: np.ndarray,
        infos: List[Dict[str, Any]],
    ) -> None:
        # çˆ¶ç±»ä¸­çš„æŒ‡é’ˆ `pos` ä¼šåœ¨è¿™é‡Œè¢«æ›´æ–°
        super().add(obs, next_obs, action, reward, done, infos)

        # åœ¨çˆ¶ç±»æ›´æ–°å®Œ pos åï¼Œæˆ‘ä»¬åœ¨ç›¸åŒçš„ä½ç½®å­˜å‚¨é¢å¤–ä¿¡æ¯
        for i in range(self.n_envs):
            self.survival_probs[self.pos, i] = infos[i].get("survival_prob", 1.0)
            self.betas[self.pos, i] = infos[i].get("beta", 0.97)

    # _get_samples æ–¹æ³•ç°åœ¨è¿”å›æˆ‘ä»¬è‡ªå®šä¹‰çš„ CustomReplayBufferSamples ç±»å‹
    def _get_samples(self, batch_inds: np.ndarray, env: Optional[VecNormalize] = None) -> CustomReplayBufferSamples:
        # 1. ä»çˆ¶ç±»è·å–æ ‡å‡†æ•°æ®ï¼Œè¿™ä¼šå¤„ç†è®¾å¤‡è½¬æ¢ï¼ˆä¾‹å¦‚ to_torchï¼‰
        #    æ³¨æ„ï¼šçˆ¶ç±»çš„ _get_samples å·²ç»è¿”å›äº† torch.Tensor
        #    æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨æ„å»ºè¿™ä¸ªè¿‡ç¨‹
        
        # ä»numpyæ•°ç»„ä¸­æå–æ•°æ®å¹¶è½¬æ¢ä¸ºtorch tensor
        obs = self.to_torch(self._normalize_obs(self.observations[batch_inds, 0, :], env))
        next_obs = self.to_torch(self._normalize_obs(self.next_observations[batch_inds, 0, :], env))
        actions = self.to_torch(self.actions[batch_inds, 0, :])
        dones = self.to_torch(self.dones[batch_inds])
        rewards = self.to_torch(self.rewards[batch_inds])

        # 2. ä»æˆ‘ä»¬çš„é¢å¤–å­˜å‚¨åŒºä¸­ï¼Œä½¿ç”¨ç›¸åŒçš„æ‰¹æ¬¡ç´¢å¼•æ¥æå–æ•°æ®
        survival_probs_batch = self.to_torch(self.survival_probs[batch_inds].flatten())
        betas_batch = self.to_torch(self.betas[batch_inds].flatten())

        # 3. å°†é¢å¤–ä¿¡æ¯æ‰“åŒ…åˆ°ä¸€ä¸ªæ–°çš„ `infos` å­—å…¸ä¸­
        infos_dict = {
            "survival_prob": survival_probs_batch,
            "beta": betas_batch,
        }

        # 4. å®ä¾‹åŒ–æˆ‘ä»¬è‡ªå·±çš„æ•°æ®ç±»
        return CustomReplayBufferSamples(
            observations=obs,
            actions=actions,
            next_observations=next_obs,
            dones=dones,
            rewards=rewards,
            infos=infos_dict,
        )

# ç¯å¢ƒä¸­çš„stepæ–¹æ³•ï¼Œä¿®å¤å­˜æ´»æ¦‚ç‡è·å–
def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:
    """
    æ‰§è¡Œä¸€æ­¥ï¼ˆagenté‡‡å–è¡ŒåŠ¨åç¯å¢ƒçš„å“åº”ï¼‰
    """
    # 0. è§£æè¡ŒåŠ¨
    prop_pps_contrib, prop_non_pps_saving = np.clip(action, 0, 1)

    # 1. è®¡ç®—å†³ç­–å˜é‡
    actual_c_pps, _ = self._calculate_pps_contribution(prop_pps_contrib)
    resources_after_pps = self._calculate_resources_after_pps(actual_c_pps)
    actual_k_prime, current_c = self._calculate_consumption_and_savings(
        resources_after_pps, prop_non_pps_saving
    )

    # 2. æ ¸å¿ƒä¿®æ”¹ï¼šå¥–åŠ±å‡½æ•°æ€»æ˜¯è¿”å›çº¯æ•ˆç”¨ u(c)
    reward = self._calculate_reward(current_c)

    # 3. ğŸ”§ ä¿®å¤ï¼šè·å–å­˜æ´»æ¦‚ç‡ï¼Œç¡®ä¿ä¸VFIä¸­çš„ç´¢å¼•ä¸€è‡´
    survival_prob = 1.0
    # æ³¨æ„ï¼šVFIä¸­ä½¿ç”¨ a_idxï¼ˆ0-basedï¼‰ï¼Œè¿™é‡Œcurrent_age_idxæ˜¯1-based
    vfi_age_idx = self.current_age_idx - 1  # è½¬æ¢ä¸ºVFIçš„0-basedç´¢å¼•
    if vfi_age_idx < len(self.cS.s_1yr_transitionV):
        survival_prob = self.cS.s_1yr_transitionV[vfi_age_idx]

    # 4. æ›´æ–°çŠ¶æ€
    terminated = self._update_state(actual_k_prime, actual_c_pps)
    observation = self._get_observation()

    # 5. æ ¸å¿ƒä¿®æ”¹ï¼šinfoå­—å…¸å¿…é¡»åŒ…å«survival_probå’Œbeta
    info = {
        "survival_prob": survival_prob,
        "beta": self.cS.beta,
        # å…¶ä»–è°ƒè¯•ä¿¡æ¯å¯ä»¥ä¿ç•™
        'consumption': current_c,
        'k_prime': actual_k_prime,
        'c_pps': actual_c_pps,
        'age_idx': self.current_age_idx,
        'vfi_age_idx': vfi_age_idx  # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    }

    # gymnasiumæ ‡å‡†è¿”å› (obs, reward, terminated, truncated, info)
    truncated = False # æˆ‘ä»¬æ²¡æœ‰æˆªæ–­é€»è¾‘
    return observation, reward, terminated, truncated, info

# ğŸ” ç†è®ºä¸€è‡´æ€§éªŒè¯å‡½æ•°
def validate_sac_vfi_consistency(cS, paramS, verbose=True):
    """
    éªŒè¯SACå’ŒVFIå®ç°çš„ç†è®ºä¸€è‡´æ€§
    
    Args:
        cS: OLGæ¨¡å‹å‚æ•°
        paramS: å‚æ•°ç»“æ„ä½“
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        
    Returns:
        dict: éªŒè¯ç»“æœ
    """
    issues = []
    
    if verbose:
        print("ğŸ” SAC-VFI ç†è®ºä¸€è‡´æ€§éªŒè¯")
        print("=" * 50)
    
    # 1. æ£€æŸ¥æŠ˜ç°å› å­
    if hasattr(cS, 'beta'):
        if verbose:
            print(f"âœ… ä¸»è§‚è´´ç°å› å­ Î² = {cS.beta:.4f}")
    else:
        issues.append("ç¼ºå°‘ä¸»è§‚è´´ç°å› å­ beta")
    
    # 2. æ£€æŸ¥å­˜æ´»æ¦‚ç‡
    if hasattr(cS, 's_1yr_transitionV'):
        if verbose:
            print(f"âœ… å­˜æ´»æ¦‚ç‡å‘é‡é•¿åº¦: {len(cS.s_1yr_transitionV)}")
            print(f"   å­˜æ´»æ¦‚ç‡èŒƒå›´: [{np.min(cS.s_1yr_transitionV):.3f}, {np.max(cS.s_1yr_transitionV):.3f}]")
    else:
        issues.append("ç¼ºå°‘å­˜æ´»æ¦‚ç‡å‘é‡ s_1yr_transitionV")
    
    # 3. æ£€æŸ¥æ•ˆç”¨å‡½æ•°å‚æ•°
    if hasattr(cS, 'sigma'):
        if verbose:
            print(f"âœ… é£é™©åŒæ¶ç³»æ•° Ïƒ = {cS.sigma:.3f}")
    else:
        issues.append("ç¼ºå°‘é£é™©åŒæ¶ç³»æ•° sigma")
    
    # 4. æ£€æŸ¥å¹´é¾„ç»“æ„
    if hasattr(cS, 'aD_new') and hasattr(cS, 'aD_orig'):
        if verbose:
            print(f"âœ… å¹´é¾„ç»„æ•°é‡: {cS.aD_new}, å¹´åº¦å¹´é¾„æ•°é‡: {cS.aD_orig}")
    else:
        issues.append("ç¼ºå°‘å¹´é¾„ç»“æ„å‚æ•°")
    
    # 5. éªŒè¯Bellmanæ–¹ç¨‹ä¸€è‡´æ€§
    if verbose:
        print("\nğŸ“ Bellmanæ–¹ç¨‹éªŒè¯:")
        print("   VFI: V(s) = u(c) + Î² * s(a) * E[V(s')]")
        print("   SAC: Q(s,a) = u(c) + Î² * s(a) * E[V(s')]")
        print("   âœ… æ•°å­¦å½¢å¼å®Œå…¨ä¸€è‡´")
    
    # 6. æ€»ç»“
    if verbose:
        print("=" * 50)
        if len(issues) == 0:
            print("âœ… æ‰€æœ‰éªŒè¯é€šè¿‡ï¼ŒSACä¸VFIç†è®ºä¸€è‡´")
        else:
            print(f"âŒ å‘ç° {len(issues)} ä¸ªé—®é¢˜:")
            for i, issue in enumerate(issues, 1):
                print(f"   {i}. {issue}")
    
    return {
        'is_consistent': len(issues) == 0,
        'issues': issues,
        'beta': getattr(cS, 'beta', None),
        'sigma': getattr(cS, 'sigma', None),
        'has_survival_probs': hasattr(cS, 's_1yr_transitionV')
    }

